<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"github.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="CS224U 笔记第二部分 Supervised sentiment analysis-SST">
<meta property="og:type" content="article">
<meta property="og:title" content="SST">
<meta property="og:url" content="https://github.com/CLearnerLee/LeeBlog/2022/05/11/SST/index.html">
<meta property="og:site_name" content="AI 自学笔记">
<meta property="og:description" content="CS224U 笔记第二部分 Supervised sentiment analysis-SST">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220508170532373.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220508170732037.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220508193603659.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220508193644130.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220508194651998.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220508194821729.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220508195450917.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220508195529241.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220508200421145.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220508200827858.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220508200902266.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220508201826099.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220508201833700.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509150652309.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509150713422.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509150814378.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509150857624.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509150922402.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509151248022.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509152733223.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509153303022.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509153430332.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509155147010.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509155334846.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509155547184.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509155645966.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509155742290.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509155848263.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509160156173.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509160610314.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509160628342.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509160646328.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509170840236.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509171104642.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509194839617.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509195603048.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509200355049.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509200647452.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509201108536.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509201807737.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509202448684.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509203252343.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509203421550.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509203622089.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509203757079.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509204233699.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509205801214.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509205852122.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509210022143.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220509210646272.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/distreps-as-features.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220511153546648.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220511154002957.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/rnn_classifier.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220511161449792.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220511161644446.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220511162118858.png">
<meta property="og:image" content="https://github.com/2022/05/11/SST/image-20220511162628067.png">
<meta property="article:published_time" content="2022-05-11T08:31:40.000Z">
<meta property="article:modified_time" content="2022-05-11T08:47:58.541Z">
<meta property="article:author" content="Lee">
<meta property="article:tag" content="markdown">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/2022/05/11/SST/image-20220508170532373.png">

<link rel="canonical" href="https://github.com/CLearnerLee/LeeBlog/2022/05/11/SST/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>SST | AI 自学笔记</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">AI 自学笔记</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags - markdown fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th - 我的第一篇博客 - CS224U 笔记 fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://github.com/CLearnerLee/LeeBlog/2022/05/11/SST/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lee">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AI 自学笔记">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          SST
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-05-11 16:31:40 / 修改时间：16:47:58" itemprop="dateCreated datePublished" datetime="2022-05-11T16:31:40+08:00">2022-05-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CS224U-%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">CS224U 笔记</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="lee-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <div class="post-description">CS224U 笔记第二部分 Supervised sentiment analysis-SST</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Supervised-sentiment-analysis"><a href="#Supervised-sentiment-analysis" class="headerlink" title="Supervised sentiment analysis"></a>Supervised sentiment analysis</h1><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><h2 id="Conceptual-challenges"><a href="#Conceptual-challenges" class="headerlink" title="Conceptual challenges"></a>Conceptual challenges</h2><p>我们如何判断哪句话表达了情感？我们又如何判断他们表达的情感极性（积极/消极）以及程度多深？</p>
<h2 id="Sentiment-analysis-in-industry"><a href="#Sentiment-analysis-in-industry" class="headerlink" title="Sentiment analysis in industry"></a>Sentiment analysis in industry</h2><ol>
<li>在真实世界中表现不佳</li>
<li>有很多的应用，但是我们真正的目标是什么？是给一个像下面一样的分析图吗（下面的结果对我们的决策可能没有任何帮助）？</li>
</ol>
<img src="/2022/05/11/SST/image-20220508170532373.png" class title="图片">
<h2 id="Affective-computing"><a href="#Affective-computing" class="headerlink" title="Affective computing"></a>Affective computing</h2><ul>
<li>情感的维度、关系和过渡</li>
</ul>
<img src="/2022/05/11/SST/image-20220508170732037.png" class title="图片">
<ul>
<li>情感计算的相关任务举例<ul>
<li>主观性 (Subjectivity): Pang and Lee 2008</li>
<li>偏见 (Bias): Recasens et al. 2013; Pryzant et al. 2020</li>
<li>态度 (Stance): Anand et al. 2011</li>
<li>仇恨言论 (Hate-speech): Nobata et al. 2016</li>
<li>带有进攻性的言语或小动作 (Microaggressions): Breitfeller et al. 2019</li>
<li>屈尊 (Condescension): Wang and Potts 2019</li>
<li>嘲讽 (Sarcasm): Khodak et al. 2017</li>
<li>欺骗和背叛 (Deception and betrayal): Niculae et al. 2015</li>
<li>网络喷子 (Online trolls): Cheng et al. 2017</li>
<li>对立 (Polarization): Gentzkow et al. 2019</li>
<li>礼貌 (Politeness): Danescu-Niculescu-Mizil et al. 2013</li>
<li>语言对齐 (Linguistic alignment): Doyle et al. 2016 例如：purchase 和 buy</li>
</ul>
</li>
</ul>
<h1 id="General-practical-tips"><a href="#General-practical-tips" class="headerlink" title="General practical tips"></a>General practical tips</h1><h2 id="Selected-sentiment-datasets"><a href="#Selected-sentiment-datasets" class="headerlink" title="Selected sentiment datasets"></a>Selected sentiment datasets</h2><p>这里只列举一些值得注意的属性，仅限于情绪分析的核心任务：</p>
<ul>
<li><p>IMDb movie reviews (50K) (Maas et al. 2011):</p>
<p><a target="_blank" rel="noopener" href="http://ai.stanford.edu/~amaas/data/sentiment/index.html">http://ai.stanford.edu/~amaas/data/sentiment/index.html</a></p>
</li>
<li><p>Datasets from Lillian Lee’s group:</p>
<p><a target="_blank" rel="noopener" href="http://www.cs.cornell.edu/home/llee/data/">http://www.cs.cornell.edu/home/llee/data/</a></p>
</li>
<li><p>Datasets from Bing Liu’s group:</p>
<p><a target="_blank" rel="noopener" href="https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html">https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</a></p>
</li>
<li><p>Amazon Customer Review data:</p>
<p><a target="_blank" rel="noopener" href="https://s3.amazonaws.com/amazon-reviews-pds/readme.html">https://s3.amazonaws.com/amazon-reviews-pds/readme.html</a></p>
</li>
<li><p>Amazon Product Data (McAuley et al. 2015; He and McAuley 2016):</p>
<p><a target="_blank" rel="noopener" href="http://jmcauley.ucsd.edu/data/amazon/">http://jmcauley.ucsd.edu/data/amazon/</a></p>
</li>
<li><p>Sentiment and social networks together (West et al. 2014)</p>
<p><a target="_blank" rel="noopener" href="http://infolab.stanford.edu/~west1/TACL2014/">http://infolab.stanford.edu/~west1/TACL2014/</a></p>
</li>
<li><p>Stanford Sentiment Treebank (SST; Socher et al. 2013)</p>
<p><a target="_blank" rel="noopener" href="https://nlp.stanford.edu/sentiment/">https://nlp.stanford.edu/sentiment/</a></p>
</li>
<li><p>DynaSent (Potts et al. 2020)</p>
<p><a href="https://github.com/cgpotts/dynasent/">https://github.com/cgpotts/dynasent/</a></p>
</li>
</ul>
<h2 id="Lexica"><a href="#Lexica" class="headerlink" title="Lexica"></a>Lexica</h2><ul>
<li><p>Bing Liu’s Opinion Lexicon: nltk.corpus.opinion_lexicon</p>
</li>
<li><p>SentiWordNet: nltk.corpus.sentiwordnet</p>
</li>
<li><p>MPQA subjectivity lexicon: <a target="_blank" rel="noopener" href="http://mpqa.cs.pitt.edu">http://mpqa.cs.pitt.edu</a></p>
</li>
<li><p>Harvard General Inquirer</p>
<ul>
<li><p>Download:</p>
<p><a target="_blank" rel="noopener" href="http://www.wjh.harvard.edu/~inquirer/spreadsheet_guide.htm">http://www.wjh.harvard.edu/~inquirer/spreadsheet_guide.htm</a></p>
</li>
<li><p>Documentation:</p>
<p><a target="_blank" rel="noopener" href="http://www.wjh.harvard.edu/~inquirer/homecat.htm">http://www.wjh.harvard.edu/~inquirer/homecat.htm</a></p>
</li>
</ul>
</li>
<li><p>Linguistic Inquiry and Word Counts (LIWC):</p>
<p><a target="_blank" rel="noopener" href="https://liwc.wpengine.com">https://liwc.wpengine.com</a></p>
</li>
<li><p>Hamilton et al. (2016): SocialSent</p>
<p><a target="_blank" rel="noopener" href="https://nlp.stanford.edu/projects/socialsent/">https://nlp.stanford.edu/projects/socialsent/</a></p>
</li>
<li><p>Brysbaert et al. (2014): Norms of valence, arousal, and dominance for 13,915 English lemmas</p>
</li>
</ul>
<h2 id="Tokenizing"><a href="#Tokenizing" class="headerlink" title="Tokenizing"></a>Tokenizing</h2><p><strong>Raw text</strong>: <code>@NLUers: can&amp;#39;t wait for the Jun 9 #projects!
YAAAAAAY!!! &amp;gt;:-D http://stanford.edu/class/cs224u/.</code></p>
<p><strong>Isolate mark-up, and replace HTML entities</strong>: <code>@NLUers: can’t wait for the Jun 9 #projects! YAAAAAAY!!! &gt;:-D http://stanford.edu/class/cs224u/</code></p>
<h3 id="Whitespace-tokenizer"><a href="#Whitespace-tokenizer" class="headerlink" title="Whitespace tokenizer"></a>Whitespace tokenizer</h3><p><strong>Raw text</strong>: <code>@NLUers: can&amp;#39;t wait for the Jun 9 #projects! YAAAAAAY!!! &amp;gt;:-D http://stanford.edu/class/cs224u/.</code></p>
<p>空格符号分割，即split(‘ ‘)，最简单的一个分词器。”isn’t”作为一个整体，没有被分割</p>
<p><code>@NLUers:</code> <code>can’t</code> <code>wait</code> <code>for</code> <code>the</code> <code>Jun</code> <code>9</code> <code>\#projects</code> <code>YAAAAAAY!!!</code> <code>*&gt;*:-D</code> <code>http://stanford.edu/class/cs224u/.</code></p>
<h3 id="Treebank-tokenizer"><a href="#Treebank-tokenizer" class="headerlink" title="Treebank tokenizer"></a>Treebank tokenizer</h3><p><strong>Raw text</strong>: <code>@NLUers: can&amp;#39;t wait for the Jun 9 #projects! YAAAAAAY!!! &amp;gt;:-D http://stanford.edu/class/cs224u/.</code></p>
<p>内置了多种常见的英语分词规则。例如，它从相邻的词条中将短语结束符号（<code>?</code> <code>!</code> <code>.</code> <code>;</code> <code>,</code>）分开，将包含句号的小数当成单个词条。另外，它还包含一些英文缩略语的规则，例如，<code>don’t</code> 会切分成[<code>do</code>,  <code>n&#39;t</code>]。</p>
<p><code>@</code> <code>NLUers</code> <code>:</code> <code>ca</code> <code>n&#39;t</code> <code>wait</code> <code>for</code> <code>the</code> <code>Jun</code> <code>9</code> <code>#</code> <code>projects</code> <code>!</code> <code>YAAAAAAY</code> <code>!</code> <code>!</code> <code>!</code> <code>&gt;</code> <code>:</code> <code>-D</code> <code>http</code> <code>:</code> <code>//stanford.edu/class/cs224u/</code> <code>.</code></p>
<h3 id="Sentiment-aware-tokenizer"><a href="#Sentiment-aware-tokenizer" class="headerlink" title="Sentiment-aware tokenizer"></a>Sentiment-aware tokenizer</h3><ul>
<li>分隔表情符号</li>
<li>尊重Twitter和其他特定领域的标记</li>
<li>使用底层标记（例如， <code>&lt;strong&gt;</code> 标签）</li>
<li>捕捉类似于 <code>\#$%ing</code></li>
<li>在有意义的地方保留大写</li>
<li>正则化加长(Regularizes lengthening)（例如，<code>YAAAAAAY</code>⇒<code>YAAAY</code>）</li>
<li>捕获重要的多字表达式（例如，<code>out of this world</code>）</li>
</ul>
<p><code>@nluers</code> <code>:</code> <code>can&#39;t</code> <code>wait</code> <code>for</code> <code>the</code> <code>Jun_9</code> <code>#projects</code> <code>!</code> <code>YAAAY</code> <code>!</code> <code>!</code> <code>!</code> <code>&gt;:-D</code> <code>http://stanford.edu/class/cs224u/</code> <code>.</code></p>
<p>A good start: <a target="_blank" rel="noopener" href="https://www.nltk.org/_modules/nltk/tokenize/casual.html">nltk.tokenize.casual.TweetTokenizer</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> TweetTokenizer</span><br><span class="line">tknzr = TweetTokenizer()</span><br><span class="line">s0 = <span class="string">&quot;This is a cooool #dummysmiley: :-) :-P &lt;3 and some arrows &lt; &gt; -&gt; &lt;--&quot;</span></span><br><span class="line">tknzr.tokenize(s0)</span><br><span class="line"><span class="comment"># out: [&#x27;This&#x27;, &#x27;is&#x27;, &#x27;a&#x27;, &#x27;cooool&#x27;, &#x27;#dummysmiley&#x27;, &#x27;:&#x27;, &#x27;:-)&#x27;, &#x27;:-P&#x27;, &#x27;&lt;3&#x27;, &#x27;and&#x27;, &#x27;some&#x27;, &#x27;arrows&#x27;, &#x27;&lt;&#x27;, &#x27;&gt;&#x27;, &#x27;-&gt;&#x27;, &#x27;&lt;--&#x27;]</span></span><br></pre></td></tr></table></figure>
<p><code>TweetTokenizer</code>与<code>word_tokenize</code>区别：</p>
<p><code>TweetTokenizer</code>保持标签完整，而<code>word_tokenize</code>没有。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> TweetTokenizer</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line">tt = TweetTokenizer()</span><br><span class="line">tweet = <span class="string">&quot;This is a cooool #dummysmiley: :-) :-P &lt;3 and some arrows &lt; &gt; -&gt; &lt;-- @remy: This is waaaaayyyy too much for you!!!!!!&quot;</span></span><br><span class="line"><span class="built_in">print</span>(tt.tokenize(tweet))</span><br><span class="line"><span class="built_in">print</span>(word_tokenize(tweet))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># [&#x27;This&#x27;, &#x27;is&#x27;, &#x27;a&#x27;, &#x27;cooool&#x27;, &#x27;#dummysmiley&#x27;, &#x27;:&#x27;, &#x27;:-)&#x27;, &#x27;:-P&#x27;, &#x27;&lt;3&#x27;, &#x27;and&#x27;, &#x27;some&#x27;, &#x27;arrows&#x27;, &#x27;&lt;&#x27;, &#x27;&gt;&#x27;, &#x27;-&gt;&#x27;, &#x27;&lt;--&#x27;, &#x27;@remy&#x27;, &#x27;:&#x27;, &#x27;This&#x27;, &#x27;is&#x27;, &#x27;waaaaayyyy&#x27;, &#x27;too&#x27;, &#x27;much&#x27;, &#x27;for&#x27;, &#x27;you&#x27;, &#x27;!&#x27;, &#x27;!&#x27;, &#x27;!&#x27;]</span></span><br><span class="line"><span class="comment"># [&#x27;This&#x27;, &#x27;is&#x27;, &#x27;a&#x27;, &#x27;cooool&#x27;, &#x27;#&#x27;, &#x27;dummysmiley&#x27;, &#x27;:&#x27;, &#x27;:&#x27;, &#x27;-&#x27;, &#x27;)&#x27;, &#x27;:&#x27;, &#x27;-P&#x27;, &#x27;&lt;&#x27;, &#x27;3&#x27;, &#x27;and&#x27;, &#x27;some&#x27;, &#x27;arrows&#x27;, &#x27;&lt;&#x27;, &#x27;&gt;&#x27;, &#x27;-&#x27;, &#x27;&gt;&#x27;, &#x27;&lt;&#x27;, &#x27;--&#x27;, &#x27;@&#x27;, &#x27;remy&#x27;, &#x27;:&#x27;, &#x27;This&#x27;, &#x27;is&#x27;, &#x27;waaaaayyyy&#x27;, &#x27;too&#x27;, &#x27;much&#x27;, &#x27;for&#x27;, &#x27;you&#x27;, &#x27;!&#x27;, &#x27;!&#x27;, &#x27;!&#x27;, &#x27;!&#x27;, &#x27;!&#x27;, &#x27;!&#x27;]</span></span><br></pre></td></tr></table></figure>
<p><code>word_tokenize</code>分割<code>#dummysmiley</code>为<code>&#39;#&#39;</code>和<code>&#39;dummysmiley&#39;</code>，而TweetTokenizer没有分割为和<code>&#39;#dummysmiley&#39;</code>。 因此， <code>TweetTokenizer</code>主要用于分析推文。</p>
<blockquote>
<p>Different types of tokenizers in NLTK: <a target="_blank" rel="noopener" href="https://chendianblog.wordpress.com/2016/11/25/different-types-of-tokenizers-in-nltk/">https://chendianblog.wordpress.com/2016/11/25/different-types-of-tokenizers-in-nltk/</a></p>
</blockquote>
<h3 id="The-impact-of-sentiment-aware-tokenizing"><a href="#The-impact-of-sentiment-aware-tokenizing" class="headerlink" title="The impact of sentiment-aware tokenizing"></a>The impact of sentiment-aware tokenizing</h3><img src="/2022/05/11/SST/image-20220508193603659.png" class title="图片">
<img src="/2022/05/11/SST/image-20220508193644130.png" class title="图片">
<h2 id="The-dangers-of-stemming"><a href="#The-dangers-of-stemming" class="headerlink" title="The dangers of stemming"></a>The dangers of stemming</h2><ul>
<li><p>Stemming is the process for reducing inflected (or some times derived) words to their stem, base or root form — generally a written word form.</p>
</li>
<li><p>词干提取会坍缩成不同的单词形式(Stemming collapses distinct word forms)</p>
</li>
<li><p>三种常用的词干提取算法(stemming algorithms)：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.nltk.org/_modules/nltk/stem/porter.html#PorterStemmer">the Porter stemmer</a></li>
<li><a target="_blank" rel="noopener" href="https://www.nltk.org/_modules/nltk/stem/lancaster.html#LancasterStemmer">the Lancaster stemmer</a></li>
<li><a target="_blank" rel="noopener" href="https://www.nltk.org/_modules/nltk/stem/wordnet.html">the WordNet stemmer</a></li>
</ul>
</li>
<li><p>Porter 和 Lancaster 破坏了太多的情感区别</p>
</li>
<li>WordNet 词干分析器几乎那么严重的问题，但它通常因为做得不够坍缩所以不值得运行它所需的资源。</li>
</ul>
<h3 id="The-Porter-stemmer"><a href="#The-Porter-stemmer" class="headerlink" title="The Porter stemmer"></a>The Porter stemmer</h3><p>启发式地识别单词后缀（结尾），并将它们剥离，并对结尾进行一些正则化</p>
<img src="/2022/05/11/SST/image-20220508194651998.png" class title="图片">
<p>Table: Sample of instances in which the Porter stemmer destroys a Harvard Inquirer Positiv/Negativ distinction.</p>
<h3 id="The-Lancaster-stemmer"><a href="#The-Lancaster-stemmer" class="headerlink" title="The Lancaster stemmer"></a>The Lancaster stemmer</h3><p>使用与 Porter 词干分析器相同的策略</p>
<img src="/2022/05/11/SST/image-20220508194821729.png" class title="图片">
<p>Table: Sample of instances in which the Lancaster stemmer destroys a Harvard Inquirer Positiv/Negativ distinction.</p>
<h3 id="The-WordNet-stemmer"><a href="#The-WordNet-stemmer" class="headerlink" title="The WordNet stemmer"></a>The WordNet stemmer</h3><p>WordNet词干分析器（NLTK）具有高精度。它需要单词-词性对。对于情感而言，它比较普遍的问题就是去除了比较形态</p>
<img src="/2022/05/11/SST/image-20220508195450917.png" class title="图片">
<p>Table: Representative examples of what WordNet stemming does and doesn’t do.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.stem.porter <span class="keyword">import</span> PorterStemmer</span><br><span class="line"><span class="keyword">from</span> nltk.stem.lancaster <span class="keyword">import</span> LancasterStemmer</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> SnowballStemmer</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> wordnet</span><br><span class="line"></span><br><span class="line">porter_stemmer = PorterStemmer()</span><br><span class="line">lancaster_stemmer = LancasterStemmer()</span><br><span class="line">snowball_stemmer = SnowballStemmer(<span class="string">&#x27;english&#x27;</span>)</span><br><span class="line">wordnet_lemmatizer = WordNetLemmatizer()</span><br><span class="line"></span><br><span class="line">words = [(<span class="string">&#x27;bottles&#x27;</span>, wordnet.NOUN), (<span class="string">&#x27;vases&#x27;</span>, wordnet.NOUN), (<span class="string">&#x27;lit&#x27;</span>, wordnet.VERB), (<span class="string">&#x27;said&#x27;</span>, wordnet.VERB), (<span class="string">&#x27;earlier&#x27;</span>, wordnet.ADJ)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word_tuple <span class="keyword">in</span> words:</span><br><span class="line">	word = word_tuple[<span class="number">0</span>]</span><br><span class="line">  	pos = word_tuple[<span class="number">1</span>]</span><br><span class="line">  	porter_stemmer.stem(word) <span class="comment"># output: &#x27;bottl&#x27;, &#x27;vase&#x27;, &#x27;lit&#x27;, &#x27;said&#x27;, &#x27;earlier&#x27;</span></span><br><span class="line">  	lancaster_stemmer.stem(word) <span class="comment"># output: &#x27;bottl&#x27;, &#x27;vas&#x27;, &#x27;lit&#x27;, &#x27;said&#x27;, &#x27;ear&#x27;</span></span><br><span class="line">  	snowball_stemmer.stem(word) <span class="comment"># output: &#x27;bottl&#x27;, &#x27;vase&#x27;, &#x27;lit&#x27;, &#x27;said&#x27;, &#x27;earlier&#x27;</span></span><br><span class="line">  	wordnet_lemmatizer.lemmatize(word) <span class="comment"># output: &#x27;bottle&#x27;, &#x27;vas&#x27;, &#x27;lit&#x27;, &#x27;said&#x27;, &#x27;earlier&#x27;</span></span><br><span class="line">  	wordnet_lemmatizer.lemmatize(word, pos=pos) <span class="comment"># output: &#x27;bottle&#x27;, &#x27;vas&#x27;, &#x27;light&#x27;, &#x27;say&#x27;, &#x27;early&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="The-impact-of-stemming"><a href="#The-impact-of-stemming" class="headerlink" title="The impact of stemming"></a>The impact of stemming</h3><img src="/2022/05/11/SST/image-20220508195529241.png" class title="图片">
<h2 id="Other-preprocessing-techniques"><a href="#Other-preprocessing-techniques" class="headerlink" title="Other preprocessing techniques"></a>Other preprocessing techniques</h2><h3 id="Part-of-speech-POS-tagging"><a href="#Part-of-speech-POS-tagging" class="headerlink" title="Part-of-speech (POS) tagging"></a>Part-of-speech (POS) tagging</h3><p> <a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/2783103?fr=aladdin">词性标注</a>: 能够根据词性的区别来辅助判断情绪</p>
<img src="/2022/05/11/SST/image-20220508200421145.png" class title="图片">
<h3 id="The-limits-of-POS-tagging"><a href="#The-limits-of-POS-tagging" class="headerlink" title="The limits of POS tagging"></a>The limits of POS tagging</h3><p>词性标注的风险在于，同一单词同一词性也有可能是不一样的情感，比如<code>mean</code>为形容词时，在形容不同的对象时既可能是积极的，也可能是消极的</p>
<p>1,424 cases where a (word, tag) pair is consistent with pos. and neg. lemma-level sentiment</p>
<img src="/2022/05/11/SST/image-20220508200827858.png" class title="图片">
<img src="/2022/05/11/SST/image-20220508200902266.png" class title="图片">
<h3 id="Simple-negation-marking"><a href="#Simple-negation-marking" class="headerlink" title="Simple negation marking"></a>Simple negation marking</h3><p>The phenomenon</p>
<ol>
<li><p>I didn’t enjoy it.</p>
</li>
<li><p>I never enjoy it.</p>
</li>
<li><p>No one enjoys it.</p>
</li>
<li><p>I have yet to enjoy it.</p>
</li>
<li><p>I don’t think I will enjoy it.</p>
</li>
</ol>
<p>有些否定词可能和动词距离比较远</p>
<p>解决方案 (Das and Chen 2001; Pang et al. 2002)：在出现在否定符号和从句级标点符号之间的每个单词后面加一个_NEG后缀</p>
<p><code>No one enjoys it.</code> $\rightarrow$ <code>no</code> <code>one_NEG</code> <code>enjoy_NEG</code> <code>it_NEG</code> <code>.</code></p>
<p><code>I don&#39;t think I will enjoy it, but I might.</code> $\rightarrow$ <code>i</code> <code>don&#39;t</code> <code>think_NEG</code> <code>i_NEG</code> <code>will_NEG</code> <code>enjoy_NEG</code> <code>it_NEG</code> <code>,</code> <code>but</code> <code>i</code> <code>might</code> <code>.</code></p>
<h3 id="The-impact-of-negation-marking"><a href="#The-impact-of-negation-marking" class="headerlink" title="The impact of negation marking"></a>The impact of negation marking</h3><img src="/2022/05/11/SST/image-20220508201826099.png" class title="图片">
<img src="/2022/05/11/SST/image-20220508201833700.png" class title="图片">
<h1 id="Stanford-Sentiment-Treebank"><a href="#Stanford-Sentiment-Treebank" class="headerlink" title="Stanford Sentiment Treebank"></a>Stanford Sentiment Treebank</h1><h2 id="SST-project-overview"><a href="#SST-project-overview" class="headerlink" title="SST project overview"></a>SST project overview</h2><ol>
<li><p>Socher et al. (2013)</p>
</li>
<li><p>Full code and data release:</p>
<p><a target="_blank" rel="noopener" href="https://nlp.stanford.edu/sentiment/">https://nlp.stanford.edu/sentiment/</a></p>
</li>
<li><p>Sentence-level corpus (10,662 sentences)</p>
</li>
<li><p>Original data from Rotten Tomatoes (Pang and Lee 2005)</p>
</li>
<li><p>Fully-labeled trees (crowdsourced labels)</p>
</li>
<li><p>The 5-way labels were extracted from workers’ slider responses.</p>
</li>
</ol>
<img src="/2022/05/11/SST/image-20220509150652309.png" class title="图片">
<img src="/2022/05/11/SST/image-20220509150713422.png" class title="图片">
<h2 id="Root-level-tasks"><a href="#Root-level-tasks" class="headerlink" title="Root-level tasks"></a>Root-level tasks</h2><p><strong>Five-way problem</strong></p>
<img src="/2022/05/11/SST/image-20220509150814378.png" class title="图片">
<p><strong>Ternary problem</strong></p>
<img src="/2022/05/11/SST/image-20220509150857624.png" class title="图片">
<p><strong>Binary problem (neutral data simply excluded)</strong></p>
<img src="/2022/05/11/SST/image-20220509150922402.png" class title="图片">
<h2 id="Train-dev-test-scenarios"><a href="#Train-dev-test-scenarios" class="headerlink" title="Train/dev/test scenarios"></a>Train/dev/test scenarios</h2><p><strong>Train</strong></p>
<p>Full examples and/or subphrases with or without repeats:</p>
<img src="/2022/05/11/SST/image-20220509151248022.png" class title="图片">
<p><strong>Dev/test</strong></p>
<p>Full sentences only.</p>
<h2 id="Code-Snips"><a href="#Code-Snips" class="headerlink" title="Code Snips"></a>Code Snips</h2><h3 id="SST-Data-readers"><a href="#SST-Data-readers" class="headerlink" title="SST Data readers"></a>SST Data readers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sentiment_reader</span>(<span class="params">src_filename, include_subtrees=<span class="literal">True</span>, dedup=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Iterator for our distribution of the SST-3 and other files in</span></span><br><span class="line"><span class="string">    that format.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    src_filename : str</span></span><br><span class="line"><span class="string">        Full path to the file to be read.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    include_subtrees : bool</span></span><br><span class="line"><span class="string">        If True, then the subtrees are returned as separate examples.</span></span><br><span class="line"><span class="string">        This affects only the train split. For dev and test, only</span></span><br><span class="line"><span class="string">        the full examples are included.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    dedup : bool</span></span><br><span class="line"><span class="string">        If True, only one copy of each (example, label) pair is included.</span></span><br><span class="line"><span class="string">        This mainly affects the train set, though there is one repeated</span></span><br><span class="line"><span class="string">        example in the dev set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Yields</span></span><br><span class="line"><span class="string">    ------</span></span><br><span class="line"><span class="string">    pd.DataFrame with columns [&#x27;example_id&#x27;, &#x27;sentence&#x27;, &#x27;label&#x27;]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    df = pd.read_csv(src_filename)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> include_subtrees:</span><br><span class="line">        df = df[df.is_subtree == <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> dedup:</span><br><span class="line">        df = df.groupby([<span class="string">&#x27;sentence&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]).apply(<span class="keyword">lambda</span> x: x.iloc[<span class="number">0</span>])</span><br><span class="line">        df = df.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_reader</span>(<span class="params">sst_home, include_subtrees=<span class="literal">False</span>, dedup=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Convenience function for reading the SST-3 train file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    src = os.path.join(sst_home, <span class="string">&#x27;sst3-train.csv&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> sentiment_reader(</span><br><span class="line">        src, include_subtrees=include_subtrees, dedup=dedup)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Root-only formulation</span></span><br><span class="line">SST_HOME = os.path.join(<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;sentiment&#x27;</span>)</span><br><span class="line">train_df = sst.train_reader(SST_HOME)</span><br><span class="line">train_df.sample(<span class="number">3</span>, random_state=<span class="number">1</span>).to_dict(orient=<span class="string">&quot;records&quot;</span>)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509152733223.png" class title="图片">
<img src="/2022/05/11/SST/image-20220509153303022.png" class title="图片">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Including subtrees</span></span><br><span class="line">subtree_train_df = sst.train_reader(SST_HOME, include_subtrees=<span class="literal">True</span>)</span><br><span class="line">subtree_train_df.shape[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># out: 318582</span></span><br><span class="line"></span><br><span class="line">subtree_train_df.head()</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509153430332.png" class title="图片">
<h3 id="SST-Tokenization"><a href="#SST-Tokenization" class="headerlink" title="SST Tokenization"></a>SST Tokenization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize.treebank <span class="keyword">import</span> TreebankWordDetokenizer</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize.treebank <span class="keyword">import</span> TreebankWordTokenizer</span><br><span class="line"></span><br><span class="line">ex = train_df.iloc[<span class="number">0</span>].sentence</span><br><span class="line"><span class="comment"># out: &quot;The Rock is destined to be the 21st Century &#x27;s new `` Conan &#x27;&#x27; and that he &#x27;s going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .&quot;</span></span><br><span class="line"></span><br><span class="line">detokenizer = TreebankWordDetokenizer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">detokenize</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">return</span> detokenizer.detokenize(s.split())</span><br><span class="line"></span><br><span class="line">detokenize(ex)</span><br><span class="line"><span class="comment"># out: &#x27;The Rock is destined to be the 21st Century\&#x27;s new &quot;Conan&quot; and that he\&#x27;s going to make a splash even greater than Arnold Schwarzenegger, Jean-Claud Van Damme or Steven Segal.&#x27;</span></span><br><span class="line"></span><br><span class="line">tokenizer = TreebankWordTokenizer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">treebank_tokenize</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer.tokenize(s)</span><br><span class="line"></span><br><span class="line">treebank_tokenize(<span class="string">&quot;The Rock isn&#x27;t the new ``Conan&#x27;&#x27; – he&#x27;s this generation&#x27;s Olivier!&quot;</span>)</span><br><span class="line"><span class="comment"># out: [&#x27;The&#x27;, &#x27;Rock&#x27;, &#x27;is&#x27;, &quot;n&#x27;t&quot;, &#x27;the&#x27;, &#x27;new&#x27;, &#x27;``&#x27;, &#x27;Conan&#x27;, &quot;&#x27;&#x27;&quot;, &#x27;–&#x27;, &#x27;he&#x27;, &quot;&#x27;s&quot;, &#x27;this&#x27;, &#x27;generation&#x27;, &quot;&#x27;s&quot;, &#x27;Olivier&#x27;, &#x27;!&#x27;]</span></span><br></pre></td></tr></table></figure>
<h1 id="DynaSent"><a href="#DynaSent" class="headerlink" title="DynaSent"></a>DynaSent</h1><h2 id="Project-overview"><a href="#Project-overview" class="headerlink" title="Project overview"></a>Project overview</h2><ul>
<li><p>Data, code, and models:</p>
<p><a href="https://github.com/cgpotts/dynasent">https://github.com/cgpotts/dynasent</a></p>
</li>
<li><p>121,634 sentences, across two rounds, each with 5 gold labels</p>
</li>
<li><p>Paper: Potts et al. 2020</p>
</li>
<li><p>Dynabench: <a target="_blank" rel="noopener" href="https://dynabench.org">https://dynabench.org</a></p>
</li>
</ul>
<h2 id="Dataset-overview"><a href="#Dataset-overview" class="headerlink" title="Dataset overview"></a>Dataset overview</h2><img src="/2022/05/11/SST/image-20220509155147010.png" class title="图片">
<h3 id="Round-1"><a href="#Round-1" class="headerlink" title="Round 1"></a>Round 1</h3><ul>
<li>Model 0: RoBERTa-based classififier</li>
</ul>
<img src="/2022/05/11/SST/image-20220509155334846.png" class title="图片">
<ul>
<li><p>Harvesting sentences: Favor sentences where the review is 1-star and Model 0 predicts positive, and where the review is 5-star and Model 0 predicts negative.</p>
</li>
<li><p>Validation</p>
</li>
</ul>
<img src="/2022/05/11/SST/image-20220509155547184.png" class title="图片">
<ul>
<li>Resulting dataset</li>
</ul>
<img src="/2022/05/11/SST/image-20220509155645966.png" class title="图片">
<ul>
<li>Model 0 versus the humans</li>
</ul>
<img src="/2022/05/11/SST/image-20220509155742290.png" class title="图片">
<ul>
<li>Randomly sampled (short) examples</li>
</ul>
<img src="/2022/05/11/SST/image-20220509155848263.png" class title="图片">
<h3 id="Round-2"><a href="#Round-2" class="headerlink" title="Round 2"></a>Round 2</h3><ul>
<li>Model 1: RoBERTa-based classififier</li>
</ul>
<img src="/2022/05/11/SST/image-20220509160156173.png" class title="图片">
<ul>
<li>Validation: Same as in Round 1.</li>
<li>Resulting dataset</li>
</ul>
<img src="/2022/05/11/SST/image-20220509160610314.png" class title="图片">
<ul>
<li>Model 1 versus the humans</li>
</ul>
<img src="/2022/05/11/SST/image-20220509160628342.png" class title="图片">
<ul>
<li>Randomly sampled (short) examples</li>
</ul>
<img src="/2022/05/11/SST/image-20220509160646328.png" class title="图片">
<h1 id="Hyperparameter-search-and-classififier-comparison"><a href="#Hyperparameter-search-and-classififier-comparison" class="headerlink" title="Hyperparameter search and classififier comparison"></a>Hyperparameter search and classififier comparison</h1><h2 id="Hyperparameter-search-Rationale"><a href="#Hyperparameter-search-Rationale" class="headerlink" title="Hyperparameter search : Rationale"></a>Hyperparameter search : Rationale</h2><ol>
<li><p>The parameters of a model are those whose values are learned as part of optimizing the model itself</p>
</li>
<li><p>The <strong>hyperparameters</strong> of a model are any settings that are set outside of this optimization. Examples:</p>
<ol>
<li><p>GloVe or LSA dimensionality</p>
</li>
<li><p>GloVe $x_{max}$ and $\alpha$</p>
</li>
<li><p>Regularization terms, hidden dimensionalities, learning rates, activation functions</p>
</li>
<li><p>Optimization methods</p>
</li>
</ol>
</li>
<li><p>Hyperparameter optimization is crucial to building a persuasive argument: every model must   be put in its best light!</p>
</li>
<li><p>All hyperparameter tuning must be done only on train and development data.</p>
</li>
</ol>
<h2 id="Classififier-comparison-Rationale"><a href="#Classififier-comparison-Rationale" class="headerlink" title="Classififier comparison: Rationale"></a>Classififier comparison: Rationale</h2><ol>
<li><p>Suppose you’ve assessed a baseline model <em>B</em> and your favored model <em>M</em>, and your chosen assessment metric favors <em>M</em>. Is <em>M</em> really better?</p>
</li>
<li><p>If the difference between <em>B</em> and <em>M</em> is clearly of practical signifificance, then you might not need to do anything beyond presenting the numbers. Still, is there variation in how <em>B</em> or <em>M</em> performs?</p>
</li>
<li><p>Demˇsar (2006) advises the Wilcoxon signed-rank test for situations in which you can afford to repeatedly assess <em>B</em> and <em>M</em> on different train/test splits. We’ll talk later in the term about the rationale for this.</p>
</li>
<li><p>For situations where you can’t repeatedly assess <em>B</em> and <em>M</em>, McNemar’s test is a reasonable alternative. It operates on the confusion matrices produced by the two models, testing the null hypothesis that the two models have the same error rate.</p>
</li>
</ol>
<h1 id="Hand-built-feature-functions"><a href="#Hand-built-feature-functions" class="headerlink" title="Hand-built feature functions"></a>Hand-built feature functions</h1><h2 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h2><ul>
<li><p>The core characteristics of the feature functions we’ll build here:</p>
<ul>
<li>They represent examples in <strong>very large, very sparse feature spaces</strong>.</li>
<li>The individual feature functions can be <strong>highly refined</strong>, drawing on expert human knowledge of the domain. </li>
<li>Taken together, these representations don’t comprehensively  represent the input examples. They just identify aspects of the inputs  that the classifier model can make good use of (we hope).</li>
</ul>
</li>
<li><p>These classifiers tend to be <strong>highly competitive</strong>. We’ll  look at more powerful deep learning models, and it  will immediately become apparent that it is very difficult to get them to measure up to well-built classifiers based in sparse feature  representations. It can be done, but it tends to require a lot of  attention to optimization details (and potentially a lot of compute  resources).</p>
</li>
</ul>
<h2 id="Feature-functions"><a href="#Feature-functions" class="headerlink" title="Feature functions"></a>Feature functions</h2><ul>
<li>Feature representation is arguably <strong>the most important step in any machine learning task</strong>. </li>
<li>We will define our feature functions as <code>dict</code>s mapping feature names (which can be any object that can be a <code>dict</code> key) to their values (which must be <code>bool</code>, <code>int</code>, or <code>float</code>). </li>
<li><p>To prepare for optimization, we will use <code>sklearn</code>‘s <a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html">DictVectorizer</a> class to turn these into matrices of features. </p>
</li>
<li><p>The <code>dict</code>-based approach gives us a lot of flexibility and frees us from having to worry about the underlying feature matrix.</p>
</li>
</ul>
<h3 id="Unigrams"><a href="#Unigrams" class="headerlink" title="Unigrams"></a>Unigrams</h3><p>A typical baseline or default feature representation in NLP or NLU is  built from unigrams. Here, those are the leaf nodes of the tree:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">unigrams_phi</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The basis for a unigrams feature function. Downcases all tokens.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    text : str</span></span><br><span class="line"><span class="string">        The example to represent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    defaultdict</span></span><br><span class="line"><span class="string">        A map from strings to their counts in `text`. (Counter maps a</span></span><br><span class="line"><span class="string">        list to a dict of counts of the elements in that list.)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> Counter(text.lower().split())</span><br><span class="line"></span><br><span class="line">example_text = <span class="string">&quot;NLU is enlightening !&quot;</span></span><br><span class="line"></span><br><span class="line">unigrams_phi(example_text)</span><br><span class="line"><span class="comment"># out: Counter(&#123;&#x27;nlu&#x27;: 1, &#x27;is&#x27;: 1, &#x27;enlightening&#x27;: 1, &#x27;!&#x27;: 1&#125;)</span></span><br></pre></td></tr></table></figure>
<h3 id="Bigrams"><a href="#Bigrams" class="headerlink" title="Bigrams"></a>Bigrams</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">START_SYMBOL = <span class="string">&quot;&lt;s&gt;&quot;</span></span><br><span class="line">END_SYMBOL = <span class="string">&quot;&lt;/s&gt;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bigrams_phi</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The basis for a bigrams feature function. Downcases all tokens.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    text : str</span></span><br><span class="line"><span class="string">        The example to represent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    defaultdict</span></span><br><span class="line"><span class="string">        A map from tuples to their counts in `text`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    toks = text.lower().split()</span><br><span class="line">    left = [START_SYMBOL] + toks</span><br><span class="line">    right = toks + [END_SYMBOL]</span><br><span class="line">    grams = <span class="built_in">list</span>(<span class="built_in">zip</span>(left, right))</span><br><span class="line">    <span class="keyword">return</span> Counter(grams)</span><br><span class="line"></span><br><span class="line">bigrams_phi(example_text)</span><br><span class="line"><span class="comment"># out: Counter(&#123;(&#x27;&lt;s&gt;&#x27;, &#x27;nlu&#x27;): 1,</span></span><br><span class="line"><span class="comment">#         (&#x27;nlu&#x27;, &#x27;is&#x27;): 1,</span></span><br><span class="line"><span class="comment">#         (&#x27;is&#x27;, &#x27;enlightening&#x27;): 1,</span></span><br><span class="line"><span class="comment">#         (&#x27;enlightening&#x27;, &#x27;!&#x27;): 1,</span></span><br><span class="line"><span class="comment">#         (&#x27;!&#x27;, &#x27;&lt;/s&gt;&#x27;): 1&#125;)</span></span><br></pre></td></tr></table></figure>
<h3 id="A-note-on-DictVectorizer"><a href="#A-note-on-DictVectorizer" class="headerlink" title="A note on DictVectorizer"></a>A note on DictVectorizer</h3><p>上述函数只是特征表示的基础。事实上，我们的模型通常不会将示例表示为字典，而是表示为嵌入矩阵中的向量。一般来说，为了管理从字典到向量的转换，我们使用 <code>sklearn.feature_extraction.DictVectorizer</code> 实例。下面简要介绍一下这些方法的工作原理：</p>
<p> 首先，假设我们只有两个示例要表示，我们的特征函数将它们映射到以下字典列表： </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_feats = [</span><br><span class="line">    &#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">1</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;b&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">2</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>现在我们创建一个<code>DictVectorizer</code>。为了更容易地检查结果矩阵，这里设置了<code>sparse=False</code>，因此返回值是稠密矩阵。对于实际问题，最好使用<code>sparse=True</code>，因为对于可能要创建的非常稀疏的特征矩阵，它将更加有效。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"></span><br><span class="line">vec = DictVectorizer(sparse=<span class="literal">False</span>)  <span class="comment"># Use `sparse=True` for real problems!</span></span><br></pre></td></tr></table></figure>
<p><code>fit_transform</code> 将我们的字典映射为矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_train = vec.fit_transform(train_feats)</span><br><span class="line"></span><br><span class="line">pd.DataFrame(X_train, columns=vec.get_feature_names())</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509170840236.png" class title="图片">
<p>现在我们可以直观地看到，第一列中嵌入了名为“a”特征，第二列中嵌入了“b”，第三列中嵌入了“c”</p>
<p>现在我们有下面检验的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">test_feats = [</span><br><span class="line">    &#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">1</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">1</span>&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># If we have trained a model on `X_train`, then it will not have any way to deal with this new feature &quot;d&quot;. This shows that we need to embed `test_feats` in the same space as `X_train`. To do this, one just calls `transform` on the existing vectorizer:</span></span><br><span class="line">X_test = vec.transform(test_feats)  <span class="comment"># Not `fit_transform`!</span></span><br><span class="line"></span><br><span class="line">pd.DataFrame(X_test, columns=vec.get_feature_names())</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509171104642.png" class title="图片">
<p><code>DictVectorizer</code>最常见的错误是在测试示例上调用<code>fit_transform</code>。这将消除现有的表示方案，用一个与测试示例匹配的方案替换它。这可能表现为与特征计数相关的<code>ValueError</code>。以下是一个可能会帮助在工作中发现这一点的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">toy_mod = LogisticRegression()</span><br><span class="line"></span><br><span class="line">vec = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">X_train = vec.fit_transform(train_feats)</span><br><span class="line"></span><br><span class="line">toy_mod.fit(X_train, [<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Here&#x27;s the error! Don&#x27;t use `fit_transform` again! Use `transform`!</span></span><br><span class="line">X_test = vec.fit_transform(test_feats)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    toy_mod.predict(X_test)</span><br><span class="line"><span class="keyword">except</span> ValueError <span class="keyword">as</span> err:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;ValueError: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(err))</span><br><span class="line"><span class="comment"># out: ValueError: X has 4 features per sample; expecting 3</span></span><br></pre></td></tr></table></figure>
<h2 id="Building-datasets-for-experiments"><a href="#Building-datasets-for-experiments" class="headerlink" title="Building datasets for experiments"></a>Building datasets for experiments</h2><p> 我们分析的第二个主要阶段是建立阶段。成分： </p>
<ul>
<li>来自一个函数的数据集，类似于<code>train.reader</code></li>
<li>一个特征函数<code>unigrams_phi</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_dataset</span>(<span class="params">dataframes, phi, vectorizer=<span class="literal">None</span>, vectorize=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Core general function for building experimental datasets.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    dataframes : pd.DataFrame or list of pd.DataFrame</span></span><br><span class="line"><span class="string">        The dataset or datasets to process, as read in by</span></span><br><span class="line"><span class="string">        `sentiment_reader`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    phi : feature function</span></span><br><span class="line"><span class="string">       Any function that takes a string as input and returns a</span></span><br><span class="line"><span class="string">       bool/int/float-valued dict as output.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    vectorizer : sklearn.feature_extraction.DictVectorizer</span></span><br><span class="line"><span class="string">       If this is None, then a new `DictVectorizer` is created and</span></span><br><span class="line"><span class="string">       used to turn the list of dicts created by `phi` into a</span></span><br><span class="line"><span class="string">       feature matrix. This happens when we are training.</span></span><br><span class="line"><span class="string">       If this is not None, then it&#x27;s assumed to be a `DictVectorizer`</span></span><br><span class="line"><span class="string">       and used to transform the list of dicts. This happens in</span></span><br><span class="line"><span class="string">       assessment, when we take in new instances and need to</span></span><br><span class="line"><span class="string">       featurize them as we did in training.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    vectorize : bool</span></span><br><span class="line"><span class="string">       Whether to use a DictVectorizer. Set this to False for</span></span><br><span class="line"><span class="string">       deep learning models that process their own input.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    dict</span></span><br><span class="line"><span class="string">        A dict with keys &#x27;X&#x27; (the feature matrix), &#x27;y&#x27; (the list of</span></span><br><span class="line"><span class="string">        labels), &#x27;vectorizer&#x27; (the `DictVectorizer`), and</span></span><br><span class="line"><span class="string">        &#x27;raw_examples&#x27; (the `nltk.Tree` objects, for error analysis).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(dataframes, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">        df = pd.concat(dataframes)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        df = dataframes</span><br><span class="line"></span><br><span class="line">    raw_examples = <span class="built_in">list</span>(df.sentence.values)</span><br><span class="line"></span><br><span class="line">    feat_dicts = <span class="built_in">list</span>(df.sentence.apply(phi).values)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;label&#x27;</span> <span class="keyword">in</span> df.columns:</span><br><span class="line">        labels = <span class="built_in">list</span>(df.label.values)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        labels = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    feat_matrix = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> vectorize:</span><br><span class="line">        <span class="comment"># In training, we want a new vectorizer:</span></span><br><span class="line">        <span class="keyword">if</span> vectorizer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            vectorizer = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">            feat_matrix = vectorizer.fit_transform(feat_dicts)</span><br><span class="line">        <span class="comment"># In assessment, we featurize using the existing vectorizer:</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            feat_matrix = vectorizer.transform(feat_dicts)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        feat_matrix = feat_dicts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;X&#x27;</span>: feat_matrix,</span><br><span class="line">            <span class="string">&#x27;y&#x27;</span>: labels,</span><br><span class="line">            <span class="string">&#x27;vectorizer&#x27;</span>: vectorizer,</span><br><span class="line">            <span class="string">&#x27;raw_examples&#x27;</span>: raw_examples&#125;</span><br><span class="line"></span><br><span class="line">train_dataset = sst.build_dataset(</span><br><span class="line">    train_reader(SST_HOME),</span><br><span class="line">    phi=unigrams_phi,</span><br><span class="line">    vectorizer=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Train dataset with unigram features has &#123;:,&#125; examples and &quot;</span></span><br><span class="line">      <span class="string">&quot;&#123;:,&#125; features.&quot;</span>.<span class="built_in">format</span>(*train_dataset[<span class="string">&#x27;X&#x27;</span>].shape))</span><br><span class="line"><span class="comment"># Train dataset with unigram features has 8,544 examples and 16,579 features.</span></span><br></pre></td></tr></table></figure>
<p>Notice that <code>build_dataset</code> has an optional argument <code>vectorizer</code>:</p>
<ul>
<li><p>If it is <code>None</code>, then a new vectorizer is used and returned as <code>dataset[&#39;vectorizer&#39;]</code>. This is the usual scenario when training. </p>
</li>
<li><p>For evaluation, one wants to represent examples exactly as they were represented during training. To ensure that this happens, pass the training <code>vectorizer</code> to this function, so that <code>transform</code> is used.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dev_dataset = sst.build_dataset(</span><br><span class="line">    sst.dev_reader(SST_HOME),</span><br><span class="line">    phi=unigrams_phi,</span><br><span class="line">    vectorizer=train_dataset[<span class="string">&#x27;vectorizer&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Dev dataset with unigram features has &#123;:,&#125; examples &quot;</span></span><br><span class="line">      <span class="string">&quot;and &#123;:,&#125; features&quot;</span>.<span class="built_in">format</span>(*dev_dataset[<span class="string">&#x27;X&#x27;</span>].shape))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dev dataset with unigram features has 1,101 examples and 16,579 features</span></span><br></pre></td></tr></table></figure>
<h2 id="Basic-optimization"><a href="#Basic-optimization" class="headerlink" title="Basic optimization"></a>Basic optimization</h2><p>The module <code>np_sgd_classifier</code> contains a complete optimization framework, as <code>BasicSGDClassifier</code>. Well, it’s complete in the sense that it achieves our full task of supervised learning. It’s incomplete in the sense that it is very basic. You probably wouldn’t want to use it in experiments. Rather, we’re going to encourage you to rely on <code>sklearn</code> for your experiments (see below). Still, this is a good basic picture of what’s happening under the hood.</p>
<p>So what is <code>BasicSGDClassifier</code> doing? The heart of it is the <code>fit</code> function (reflecting the usual <code>sklearn</code> naming system). This method implements a hinge-loss stochastic sub-gradient descent optimization. Intuitively, it works as follows:</p>
<ol>
<li>Start by assuming that all the feature weights are <code>0</code>.</li>
<li>Move through the dataset instance-by-instance in random order.</li>
<li>For each instance, classify it using the current weights. </li>
<li>If the classification is incorrect, move the weights in the direction of the correct classification</li>
</ol>
<p>This process repeats for a user-specified number of iterations (default <code>10</code> below), and the weight movement is tempered by a learning-rate parameter <code>eta</code> (default <code>0.1</code>). The output is a set of weights that can be used to make predictions about new (properly featurized) examples.</p>
<p>In more technical terms, the objective function is </p>
<script type="math/tex; mode=display">
  \min_{\mathbf{w} \in \mathbb{R}^{d}}
  \sum_{(x,y)\in\mathcal{D}} 
  \max_{y'\in\mathbf{Y}}
  \left[\mathbf{Score}_{\textbf{w}, \phi}(x,y') + \mathbf{cost}(y,y')\right] - \mathbf{Score}_{\textbf{w}, \phi}(x,y)</script><p>where $\mathbf{w}$ is the set of weights to be learned, $\mathcal{D}$ is the training set of example&ndash;label pairs, $\mathbf{Y}$ is the set of labels, $\mathbf{cost}(y,y’) = 0$ if $y=y’$, else $1$, and $\mathbf{Score}_{\textbf{w}, \phi}(x,y’)$ is the inner product of the weights<br>$\mathbf{w}$ and the example as featurized according to $\phi$.</p>
<p>The <code>fit</code> method is then calculating the sub-gradient of this objective. In succinct pseudo-code:</p>
<ul>
<li>Initialize $\mathbf{w} = \mathbf{0}$</li>
<li>Repeat $T$ times:<ul>
<li>for each $(x,y) \in \mathcal{D}$ (in random order):<ul>
<li>$\tilde{y} = \text{argmax}_{y’\in \mathcal{Y}} \mathbf{Score}_{\textbf{w}, \phi}(x,y’) + \mathbf{cost}(y,y’)$</li>
<li>$\mathbf{w} =  \mathbf{w} + \eta(\phi(x,y) - \phi(x,\tilde{y}))$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>This is very intuitive – push the weights in the direction of the positive cases. It doesn’t require any probability theory. And such loss functions have proven highly effective in many settings. For a more powerful version of this classifier, see <a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier">sklearn.linear_model.SGDClassifier</a>. With <code>loss=&#39;hinge&#39;</code>, it should behave much like <code>BasicSGDClassifier</code> (but faster!).</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BasicSGDClassifier</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">safe_macro_f1</span>(<span class="params">y, y_pred</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Macro-averaged F1, forcing `sklearn` to report as a multiclass</span></span><br><span class="line"><span class="string">    problem even when there are just two classes. `y` is the list of</span></span><br><span class="line"><span class="string">    gold labels and `y_pred` is the list of predicted labels.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> f1_score(y, y_pred, average=<span class="string">&#x27;macro&#x27;</span>, pos_label=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicSGDClassifier</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, max_iter=<span class="number">10</span>, eta=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Basic implementation hinge-loss stochastic sub-gradient descent</span></span><br><span class="line"><span class="string">        optimization, intended to illustrate the basic concepts of</span></span><br><span class="line"><span class="string">        classifier optimization in code.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        max_iter : int (default: 10)</span></span><br><span class="line"><span class="string">            Number of training epochs (full runs through shuffled data).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        eta : float (default: 0.1)</span></span><br><span class="line"><span class="string">            Learning rate parameter.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.max_iter = max_iter</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.params = [<span class="string">&#x27;max_iter&#x27;</span>, <span class="string">&#x27;eta&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, feat_matrix, labels</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Core optimization function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        feat_matrix : 2d matrix (np.array or any scipy.sparse type)</span></span><br><span class="line"><span class="string">            The design matrix, one row per example. Hence, the row</span></span><br><span class="line"><span class="string">            dimensionality is the example count and the column</span></span><br><span class="line"><span class="string">            dimensionality is number of features.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        labels : list</span></span><br><span class="line"><span class="string">            The labels for each example, hence assumed to have the</span></span><br><span class="line"><span class="string">            same length as, and be aligned with, `feat_matrix`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        For attributes, we follow the `sklearn` style of using a</span></span><br><span class="line"><span class="string">        final `_` for attributes that are created by `fit` methods:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Attributes</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        self.classes_ : list</span></span><br><span class="line"><span class="string">            The set of class labels in sorted order.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        self.n_classes_ : int</span></span><br><span class="line"><span class="string">            Length of `self.classes_`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        self.coef_ : np.array of dimension (class count, feature count)</span></span><br><span class="line"><span class="string">            These are the weights, named as in `sklearn`. They are</span></span><br><span class="line"><span class="string">            organized so that each row represents the feature weights</span></span><br><span class="line"><span class="string">            for a given class, as is typical in `sklearn`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># We&#x27;ll deal with the labels via their indices into self.classes_:</span></span><br><span class="line">        self.classes_ = <span class="built_in">sorted</span>(<span class="built_in">set</span>(labels))</span><br><span class="line">        self.n_classes_ = <span class="built_in">len</span>(self.classes_)</span><br><span class="line">        <span class="comment"># Useful dimensions to store:</span></span><br><span class="line">        examplecount, featcount = feat_matrix.shape</span><br><span class="line">        <span class="comment"># The weight matrix -- classes by row:</span></span><br><span class="line">        self.coef_ = np.zeros((self.n_classes_, featcount))</span><br><span class="line">        <span class="comment"># Indices for shuffling the data at the start of each epoch:</span></span><br><span class="line">        indices = <span class="built_in">list</span>(<span class="built_in">range</span>(examplecount))</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line">            random.shuffle(indices)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> indices:</span><br><span class="line">                <span class="comment"># Training instance as a feature rep and a label index:</span></span><br><span class="line">                rep = feat_matrix[i]</span><br><span class="line">                label_index = self.classes_.index(labels[i])</span><br><span class="line">                <span class="comment"># Costs are 1.0 except for the true label:</span></span><br><span class="line">                costs = np.ones(self.n_classes_)</span><br><span class="line">                costs[label_index] = <span class="number">0.0</span></span><br><span class="line">                <span class="comment"># Make a prediction:</span></span><br><span class="line">                predicted_index = self.predict_one(rep, costs=costs)</span><br><span class="line">                <span class="comment"># Weight update if it&#x27;s an incorrect prediction:</span></span><br><span class="line">                <span class="keyword">if</span> predicted_index != label_index:</span><br><span class="line">                    self.coef_[label_index] += self.eta * rep</span><br><span class="line">                    self.coef_[predicted_index] += self.eta * -rep</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict_one</span>(<span class="params">self, rep, costs=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        The core classification function. The code just needs to</span></span><br><span class="line"><span class="string">        figure out which class is highest scoring and make a random</span></span><br><span class="line"><span class="string">        choice from that set (in case of ties).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        rep : np.array of dimension featcount or</span></span><br><span class="line"><span class="string">              `scipy.sparse` matrix of dimension (1 x `featcount`)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        costs : float or np.array of dimension self.classcount</span></span><br><span class="line"><span class="string">            Where this is 0.0, we&#x27;re doing prediction. Where it</span></span><br><span class="line"><span class="string">            is an array, we expect a 0.0 at the coordinate</span></span><br><span class="line"><span class="string">            corresponding to the true label and a 1.0 in all</span></span><br><span class="line"><span class="string">            other positions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        int</span></span><br><span class="line"><span class="string">            The index of the correct class. This is for the</span></span><br><span class="line"><span class="string">            sake of the `fit` method. `predict` returns the class</span></span><br><span class="line"><span class="string">            names themselves.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        scores = rep.dot(self.coef_.T) + costs</span><br><span class="line">        <span class="comment"># Manage the difference between scipy and numpy 1d matrices:</span></span><br><span class="line">        scores = scores.reshape(self.n_classes_)</span><br><span class="line">        <span class="comment"># Set of highest scoring label indices (in case of ties):</span></span><br><span class="line">        candidates = np.argwhere(scores==np.<span class="built_in">max</span>(scores)).flatten()</span><br><span class="line">        <span class="keyword">return</span> random.choice(candidates)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        preds = self.predict(X)</span><br><span class="line">        <span class="keyword">return</span> safe_macro_f1(y, preds)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, reps</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Batch prediction function for experiments.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        reps : list or feature matrix</span></span><br><span class="line"><span class="string">           A featurized set of examples to make predictions about.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        list of str</span></span><br><span class="line"><span class="string">            A list of class names -- the predictions. Unlike `predict_one`,</span></span><br><span class="line"><span class="string">            it returns the class name rather than its index.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> [self.classes_[self.predict_one(rep)] <span class="keyword">for</span> rep <span class="keyword">in</span> reps]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">self, deep=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Gets the hyperparameters for the model, as given by the</span></span><br><span class="line"><span class="string">        `self.params` attribute. This is called `get_params` for</span></span><br><span class="line"><span class="string">        compatibility with sklearn.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        dict</span></span><br><span class="line"><span class="string">            Map from attribute names to their values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> &#123;p: <span class="built_in">getattr</span>(self, p) <span class="keyword">for</span> p <span class="keyword">in</span> self.params&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_params</span>(<span class="params">self, **params</span>):</span><br><span class="line">        <span class="keyword">for</span> key, val <span class="keyword">in</span> params.items():</span><br><span class="line">            <span class="built_in">setattr</span>(self, key, val)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simple_example</span>():</span><br><span class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line">    <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report, accuracy_score</span><br><span class="line"></span><br><span class="line">    digits = load_digits()</span><br><span class="line">    X = digits.data</span><br><span class="line">    y = digits.target</span><br><span class="line"></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">        X, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">    mod = BasicSGDClassifier(max_iter=<span class="number">500</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(mod)</span><br><span class="line"></span><br><span class="line">    mod.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    predictions = mod.predict(X_test)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(classification_report(y_test, predictions))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mod.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    simple_example()</span><br></pre></td></tr></table></figure>
<h3 id="Wrapper-for-SGDClassifier"><a href="#Wrapper-for-SGDClassifier" class="headerlink" title="Wrapper for SGDClassifier"></a>Wrapper for SGDClassifier</h3><p>For the sake of our experimental framework, a simple wrapper for <code>SGDClassifier</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit_basic_sgd_classifier</span>(<span class="params">X, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Wrapper for `BasicSGDClassifier`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X : np.array, shape `(n_examples, n_features)`</span></span><br><span class="line"><span class="string">        The matrix of features, one example per row.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    y : list</span></span><br><span class="line"><span class="string">        The list of labels for rows in `X`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    BasicSGDClassifier</span></span><br><span class="line"><span class="string">        A trained `BasicSGDClassifier` instance.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    mod = BasicSGDClassifier()</span><br><span class="line">    mod.fit(X, y)</span><br><span class="line">    <span class="keyword">return</span> mod</span><br></pre></td></tr></table></figure>
<p>We now have all the pieces needed to run experiments. And <strong>we’re going to want to run a lot of experiments</strong>, trying out different feature functions, taking different perspectives on the data and labels, and using different models. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_reader</span>(<span class="params">sst_home, include_subtrees=<span class="literal">False</span>, dedup=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Convenience function for reading the SST-3 train file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    src = os.path.join(sst_home, <span class="string">&#x27;sst3-train.csv&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> sentiment_reader(</span><br><span class="line">        src, include_subtrees=include_subtrees, dedup=dedup)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dev_reader</span>(<span class="params">sst_home, include_subtrees=<span class="literal">False</span>, dedup=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Convenience function for reading the SST-3 dev file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    src = os.path.join(sst_home, <span class="string">&#x27;sst3-dev.csv&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> sentiment_reader(</span><br><span class="line">        src, include_subtrees=include_subtrees, dedup=dedup)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">experiment</span>(<span class="params"></span></span><br><span class="line"><span class="params">        train_dataframes,</span></span><br><span class="line"><span class="params">        phi,</span></span><br><span class="line"><span class="params">        train_func,</span></span><br><span class="line"><span class="params">        assess_dataframes=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        train_size=<span class="number">0.7</span>,</span></span><br><span class="line"><span class="params">        score_func=utils.safe_macro_f1,</span></span><br><span class="line"><span class="params">        vectorize=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        verbose=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        random_state=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Generic experimental framework. Either assesses with a random</span></span><br><span class="line"><span class="string">    train/test split of `train_reader` or with `assess_reader` if</span></span><br><span class="line"><span class="string">    it is given.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    train_dataframes : pd.DataFrame or list of pd.DataFrame</span></span><br><span class="line"><span class="string">        The dataset or datasets to process, as read in by</span></span><br><span class="line"><span class="string">        `sentiment_reader`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    phi : feature function</span></span><br><span class="line"><span class="string">        Any function that takes an `nltk.Tree` instance as input</span></span><br><span class="line"><span class="string">        and returns a bool/int/float-valued dict as output.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    train_func : model wrapper</span></span><br><span class="line"><span class="string">        Any function that takes a feature matrix and a label list</span></span><br><span class="line"><span class="string">        as its values and returns a fitted model with a `predict`</span></span><br><span class="line"><span class="string">        function that operates on feature matrices.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    assess_dataframes : pd.DataFrame, list of pd.DataFrame or None</span></span><br><span class="line"><span class="string">        If None, then the df from `train_dataframes` is split into</span></span><br><span class="line"><span class="string">        a random train/test split, with the the train percentage</span></span><br><span class="line"><span class="string">        determined by `train_size`. If not None, then this should</span></span><br><span class="line"><span class="string">        be a dataset or datasets to process, as read in by</span></span><br><span class="line"><span class="string">        `sentiment_reader`. Each such dataset will be read and</span></span><br><span class="line"><span class="string">        used in a separate evaluation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    train_size : float (default: 0.7)</span></span><br><span class="line"><span class="string">        If `assess_reader` is None, then this is the percentage of</span></span><br><span class="line"><span class="string">        `train_reader` devoted to training. If `assess_reader` is</span></span><br><span class="line"><span class="string">        not None, then this value is ignored.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    score_metric : function name (default: `utils.safe_macro_f1`)</span></span><br><span class="line"><span class="string">        This should be an `sklearn.metrics` scoring function. The</span></span><br><span class="line"><span class="string">        default is weighted average F1 (macro-averaged F1). For</span></span><br><span class="line"><span class="string">        comparison with the SST literature, `accuracy_score` might</span></span><br><span class="line"><span class="string">        be used instead. For other metrics that can be used here,</span></span><br><span class="line"><span class="string">        see http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    vectorize : bool</span></span><br><span class="line"><span class="string">        Whether to use a DictVectorizer. Set this to False for</span></span><br><span class="line"><span class="string">        deep learning models that process their own input.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    verbose : bool (default: True)</span></span><br><span class="line"><span class="string">        Whether to print out the model assessment to standard output.</span></span><br><span class="line"><span class="string">        Set to False for statistical testing via repeated runs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    random_state : int or None</span></span><br><span class="line"><span class="string">        Optionally set the random seed for consistent sampling</span></span><br><span class="line"><span class="string">        where random train/test splits are being created.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Prints</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    To standard output, if `verbose=True`</span></span><br><span class="line"><span class="string">        Model precision/recall/F1 report. Accuracy is micro-F1 and is</span></span><br><span class="line"><span class="string">        reported because many SST papers report that figure, but macro</span></span><br><span class="line"><span class="string">        precision/recall/F1 is better given the class imbalances and the</span></span><br><span class="line"><span class="string">        fact that performance across the classes can be highly variable.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    dict with keys</span></span><br><span class="line"><span class="string">        &#x27;model&#x27;: trained model</span></span><br><span class="line"><span class="string">        &#x27;phi&#x27;: the function used for featurization</span></span><br><span class="line"><span class="string">        &#x27;train_dataset&#x27;: a dataset as returned by `build_dataset`</span></span><br><span class="line"><span class="string">        &#x27;assess_datasets&#x27;: list of datasets as returned by `build_dataset`</span></span><br><span class="line"><span class="string">        &#x27;predictions&#x27;: list of lists of predictions on the assessment datasets</span></span><br><span class="line"><span class="string">        &#x27;metric&#x27;: `score_func.__name__`</span></span><br><span class="line"><span class="string">        &#x27;score&#x27;: the `score_func` score on each of the assessment datasets</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Train dataset:</span></span><br><span class="line">    train = build_dataset(</span><br><span class="line">        train_dataframes,</span><br><span class="line">        phi,</span><br><span class="line">        vectorizer=<span class="literal">None</span>,</span><br><span class="line">        vectorize=vectorize)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manage the assessment set-up:</span></span><br><span class="line">    X_train = train[<span class="string">&#x27;X&#x27;</span>]</span><br><span class="line">    y_train = train[<span class="string">&#x27;y&#x27;</span>]</span><br><span class="line">    raw_train = train[<span class="string">&#x27;raw_examples&#x27;</span>]</span><br><span class="line">    assess_datasets = []</span><br><span class="line">    <span class="keyword">if</span> assess_dataframes <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        X_train, X_assess, y_train, y_assess, raw_train, raw_assess = train_test_split(</span><br><span class="line">            X_train, y_train, raw_train,</span><br><span class="line">            train_size=train_size,</span><br><span class="line">            test_size=<span class="literal">None</span>,</span><br><span class="line">            random_state=random_state)</span><br><span class="line">        assess_datasets.append(&#123;</span><br><span class="line">            <span class="string">&#x27;X&#x27;</span>: X_assess,</span><br><span class="line">            <span class="string">&#x27;y&#x27;</span>: y_assess,</span><br><span class="line">            <span class="string">&#x27;vectorizer&#x27;</span>: train[<span class="string">&#x27;vectorizer&#x27;</span>],</span><br><span class="line">            <span class="string">&#x27;raw_examples&#x27;</span>: raw_assess&#125;)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(assess_dataframes, (<span class="built_in">tuple</span>, <span class="built_in">list</span>)):</span><br><span class="line">            assess_dataframes = [assess_dataframes]</span><br><span class="line">        <span class="keyword">for</span> assess_df <span class="keyword">in</span> assess_dataframes:</span><br><span class="line">            <span class="comment"># Assessment dataset using the training vectorizer:</span></span><br><span class="line">            assess = build_dataset(</span><br><span class="line">                assess_df,</span><br><span class="line">                phi,</span><br><span class="line">                vectorizer=train[<span class="string">&#x27;vectorizer&#x27;</span>],</span><br><span class="line">                vectorize=vectorize)</span><br><span class="line">            assess_datasets.append(assess)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Train:</span></span><br><span class="line">    mod = train_func(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predictions if we have labels:</span></span><br><span class="line">    predictions = []</span><br><span class="line">    scores = []</span><br><span class="line">    <span class="keyword">for</span> dataset_num, assess <span class="keyword">in</span> <span class="built_in">enumerate</span>(assess_datasets, start=<span class="number">1</span>):</span><br><span class="line">        preds = mod.predict(assess[<span class="string">&#x27;X&#x27;</span>])</span><br><span class="line">        <span class="keyword">if</span> assess[<span class="string">&#x27;y&#x27;</span>] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            predictions.append(<span class="literal">None</span>)</span><br><span class="line">            scores.append(<span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> verbose:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(assess_datasets) &gt; <span class="number">1</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;Assessment dataset &#123;&#125;&quot;</span>.<span class="built_in">format</span>(dataset_num))</span><br><span class="line">                <span class="built_in">print</span>(classification_report(assess[<span class="string">&#x27;y&#x27;</span>], preds, digits=<span class="number">3</span>))</span><br><span class="line">            predictions.append(preds)</span><br><span class="line">            scores.append(score_func(assess[<span class="string">&#x27;y&#x27;</span>], preds))</span><br><span class="line">    true_scores = [s <span class="keyword">for</span> s <span class="keyword">in</span> scores <span class="keyword">if</span> s <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(true_scores) &gt; <span class="number">1</span> <span class="keyword">and</span> verbose:</span><br><span class="line">        mean_score = np.mean(true_scores)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Mean of macro-F1 scores: &#123;0:.03f&#125;&quot;</span>.<span class="built_in">format</span>(mean_score))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the overall scores and other experimental info:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&#x27;model&#x27;</span>: mod,</span><br><span class="line">        <span class="string">&#x27;phi&#x27;</span>: phi,</span><br><span class="line">        <span class="string">&#x27;train_dataset&#x27;</span>: train,</span><br><span class="line">        <span class="string">&#x27;assess_datasets&#x27;</span>: assess_datasets,</span><br><span class="line">        <span class="string">&#x27;predictions&#x27;</span>: predictions,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: score_func.__name__,</span><br><span class="line">        <span class="string">&#x27;scores&#x27;</span>: scores&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">_ = experiment(</span><br><span class="line">    train_reader(SST_HOME),</span><br><span class="line">    unigrams_phi,</span><br><span class="line">    fit_basic_sgd_classifier,</span><br><span class="line">    assess_dataframes=dev_reader(SST_HOME),</span><br><span class="line">    train_size=<span class="number">0.7</span>,</span><br><span class="line">    score_func=safe_macro_f1,</span><br><span class="line">    verbose=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509194839617.png" class title="图片">
<p>A few notes on this function call:</p>
<ul>
<li><p>Since <code>assess_dataframes=None</code>, the function reports performance on a random train–test split from <code>train_dataframes</code>, as given by the first argument. Give <code>sst.dev_reader(SST_HOME)</code> as the argument to assess against the <code>dev</code> set.</p>
</li>
<li><p><code>unigrams_phi</code> is the function we defined above. By changing/expanding this function, you can start to improve on the above baseline, perhaps periodically seeing how you do on the dev set.</p>
</li>
<li><p><code>fit_basic_sgd_classifier</code> is the wrapper we defined above. To assess new models, simply define more functions like this one. Such functions just need to consume an <code>(X, y)</code> pair constituting a dataset and return a model.</p>
</li>
</ul>
<h3 id="Wrapper-for-LogisticRegression"><a href="#Wrapper-for-LogisticRegression" class="headerlink" title="Wrapper for LogisticRegression"></a>Wrapper for LogisticRegression</h3><p>Here’s a simple wrapper for <a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">sklearn.linear.model.LogisticRegression</a>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit_softmax_classifier</span>(<span class="params">X, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Wrapper for `sklearn.linear.model.LogisticRegression`. This is</span></span><br><span class="line"><span class="string">    also called a Maximum Entropy (MaxEnt) Classifier, which is more</span></span><br><span class="line"><span class="string">    fitting for the multiclass case.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X : np.array, shape `(n_examples, n_features)`</span></span><br><span class="line"><span class="string">        The matrix of features, one example per row.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    y : list</span></span><br><span class="line"><span class="string">        The list of labels for rows in `X`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    sklearn.linear.model.LogisticRegression</span></span><br><span class="line"><span class="string">        A trained `LogisticRegression` instance.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    mod = LogisticRegression(</span><br><span class="line">        fit_intercept=<span class="literal">True</span>,</span><br><span class="line">        solver=<span class="string">&#x27;liblinear&#x27;</span>,</span><br><span class="line">        multi_class=<span class="string">&#x27;auto&#x27;</span>)</span><br><span class="line">    mod.fit(X, y)</span><br><span class="line">    <span class="keyword">return</span> mod</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_ = experiment(</span><br><span class="line">    train_reader(SST_HOME),</span><br><span class="line">    unigrams_phi,</span><br><span class="line">    fit_softmax_classifier)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509195603048.png" class title="图片">
<h3 id="Wrapper-for-TorchShallowNeuralClassifier"><a href="#Wrapper-for-TorchShallowNeuralClassifier" class="headerlink" title="Wrapper for TorchShallowNeuralClassifier"></a>Wrapper for TorchShallowNeuralClassifier</h3><p>While we’re at it, we might as well start to get a sense for whether adding a hidden layer to our softmax classifier yields any benefits. Whereas <code>LogisticRegression</code> is, at its core, computing</p>
<script type="math/tex; mode=display">
\begin{align*}
y &= \textbf{softmax}(xW_{xy} + b_{y})
\end{align*}</script><p>the shallow neural network inserts a hidden layer with a non-linear activation applied to it:</p>
<script type="math/tex; mode=display">
\begin{align*}
h &= \tanh(xW_{xh} + b_{h}) \\
y &= \textbf{softmax}(hW_{hy} + b_{y})
\end{align*}</script><p>Here’s an illustrative example using <code>TorchShallowNeuralClassifier</code>, which is in the course repo’s <code>torch_shallow_neural_classifier.py</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch_shallow_neural_classifier.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">from</span> torch_model_base <span class="keyword">import</span> TorchModelBase</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TorchShallowNeuralClassifier</span>(<span class="title class_ inherited__">TorchModelBase</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">            hidden_dim=<span class="number">50</span>,</span></span><br><span class="line"><span class="params">            hidden_activation=nn.Tanh(<span class="params"></span>),</span></span><br><span class="line"><span class="params">            **base_kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        A model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        h = f(xW_xh + b_h)</span></span><br><span class="line"><span class="string">        y = softmax(hW_hy + b_y)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        with a cross-entropy loss and f determined by `hidden_activation`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        hidden_dim : int</span></span><br><span class="line"><span class="string">            Dimensionality of the hidden layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        hidden_activation : nn.Module</span></span><br><span class="line"><span class="string">            The non-activation function used by the network for the</span></span><br><span class="line"><span class="string">            hidden layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        **base_kwargs</span></span><br><span class="line"><span class="string">            For details, see `torch_model_base.py`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Attributes</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        loss: nn.CrossEntropyLoss(reduction=&quot;mean&quot;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        self.params: list</span></span><br><span class="line"><span class="string">            Extends TorchModelBase.params with names for all of the</span></span><br><span class="line"><span class="string">            arguments for this class to support tuning of these values</span></span><br><span class="line"><span class="string">            using `sklearn.model_selection` tools.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.hidden_activation = hidden_activation</span><br><span class="line">        <span class="built_in">super</span>().__init__(**base_kwargs)</span><br><span class="line">        self.loss = nn.CrossEntropyLoss(reduction=<span class="string">&quot;mean&quot;</span>)</span><br><span class="line">        self.params += [<span class="string">&#x27;hidden_dim&#x27;</span>, <span class="string">&#x27;hidden_activation&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_graph</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Define the model&#x27;s computation graph.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        nn.Module</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.Linear(self.input_dim, self.hidden_dim),</span><br><span class="line">            self.hidden_activation,</span><br><span class="line">            nn.Linear(self.hidden_dim, self.n_classes_))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_dataset</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Define datasets for the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : iterable of length `n_examples`</span></span><br><span class="line"><span class="string">           Each element must have the same length.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y: None or iterable of length `n_examples`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Attributes</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        input_dim : int</span></span><br><span class="line"><span class="string">            Set based on `X.shape[1]` after `X` has been converted to</span></span><br><span class="line"><span class="string">            `np.array`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        torch.utils.data.TensorDataset` Where `y=None`, the dataset will</span></span><br><span class="line"><span class="string">        yield single tensors `X`. Where `y` is specified, it will yield</span></span><br><span class="line"><span class="string">        `(X, y)` pairs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        X = np.array(X)</span><br><span class="line">        self.input_dim = X.shape[<span class="number">1</span>]</span><br><span class="line">        X = torch.FloatTensor(X)</span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            dataset = torch.utils.data.TensorDataset(X)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.classes_ = <span class="built_in">sorted</span>(<span class="built_in">set</span>(y))</span><br><span class="line">            self.n_classes_ = <span class="built_in">len</span>(self.classes_)</span><br><span class="line">            class2index = <span class="built_in">dict</span>(<span class="built_in">zip</span>(self.classes_, <span class="built_in">range</span>(self.n_classes_)))</span><br><span class="line">            y = [class2index[label] <span class="keyword">for</span> label <span class="keyword">in</span> y]</span><br><span class="line">            y = torch.tensor(y)</span><br><span class="line">            dataset = torch.utils.data.TensorDataset(X, y)</span><br><span class="line">        <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">self, X, y, device=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Uses macro-F1 as the score function. Note: this departs from</span></span><br><span class="line"><span class="string">        `sklearn`, where classifiers use accuracy as their scoring</span></span><br><span class="line"><span class="string">        function. Using macro-F1 is more consistent with our course.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        This function can be used to evaluate models, but its primary</span></span><br><span class="line"><span class="string">        use is in cross-validation and hyperparameter tuning.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X: np.array, shape `(n_examples, n_features)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y: iterable, shape `len(n_examples)`</span></span><br><span class="line"><span class="string">            These can be the raw labels. They will converted internally</span></span><br><span class="line"><span class="string">            as needed. See `build_dataset`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        device: str or None</span></span><br><span class="line"><span class="string">            Allows the user to temporarily change the device used</span></span><br><span class="line"><span class="string">            during prediction. This is useful if predictions require a</span></span><br><span class="line"><span class="string">            lot of memory and so are better done on the CPU. After</span></span><br><span class="line"><span class="string">            prediction is done, the model is returned to `self.device`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        float</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        preds = self.predict(X, device=device)</span><br><span class="line">        <span class="keyword">return</span> safe_macro_f1(y, preds)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict_proba</span>(<span class="params">self, X, device=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Predicted probabilities for the examples in `X`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : np.array, shape `(n_examples, n_features)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        device: str or None</span></span><br><span class="line"><span class="string">            Allows the user to temporarily change the device used</span></span><br><span class="line"><span class="string">            during prediction. This is useful if predictions require a</span></span><br><span class="line"><span class="string">            lot of memory and so are better done on the CPU. After</span></span><br><span class="line"><span class="string">            prediction is done, the model is returned to `self.device`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        np.array, shape `(len(X), self.n_classes_)`</span></span><br><span class="line"><span class="string">            Each row of this matrix will sum to 1.0.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        preds = self._predict(X, device=device)</span><br><span class="line">        probs = torch.softmax(preds, dim=<span class="number">1</span>).cpu().numpy()</span><br><span class="line">        <span class="keyword">return</span> probs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X, device=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Predicted labels for the examples in `X`. These are converted</span></span><br><span class="line"><span class="string">        from the integers that PyTorch needs back to their original</span></span><br><span class="line"><span class="string">        values in `self.classes_`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : np.array, shape `(n_examples, n_features)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        device: str or None</span></span><br><span class="line"><span class="string">            Allows the user to temporarily change the device used</span></span><br><span class="line"><span class="string">            during prediction. This is useful if predictions require a</span></span><br><span class="line"><span class="string">            lot of memory and so are better done on the CPU. After</span></span><br><span class="line"><span class="string">            prediction is done, the model is returned to `self.device`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        list, length len(X)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        probs = self.predict_proba(X, device=device)</span><br><span class="line">        <span class="keyword">return</span> [self.classes_[i] <span class="keyword">for</span> i <span class="keyword">in</span> probs.argmax(axis=<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simple_example</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Assess on the digits dataset.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line">    <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report, accuracy_score</span><br><span class="line"></span><br><span class="line">    utils.fix_random_seeds()</span><br><span class="line"></span><br><span class="line">    digits = load_digits()</span><br><span class="line">    X = digits.data</span><br><span class="line">    y = digits.target</span><br><span class="line"></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">        X, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">    mod = TorchShallowNeuralClassifier()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(mod)</span><br><span class="line"></span><br><span class="line">    mod.fit(X_train, y_train)</span><br><span class="line">    preds = mod.predict(X_test)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nClassification report:&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(classification_report(y_test, preds))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> accuracy_score(y_test, preds)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    simple_example()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit_nn_classifier</span>(<span class="params">X, y</span>):</span><br><span class="line">    mod = TorchShallowNeuralClassifier(</span><br><span class="line">        hidden_dim=<span class="number">100</span>,</span><br><span class="line">        early_stopping=<span class="literal">True</span>,      <span class="comment"># A basic early stopping set-up.</span></span><br><span class="line">        validation_fraction=<span class="number">0.1</span>,  <span class="comment"># If no improvement on the</span></span><br><span class="line">        tol=<span class="number">1e-5</span>,                 <span class="comment"># validation set is seen within</span></span><br><span class="line">        n_iter_no_change=<span class="number">10</span>)      <span class="comment"># `n_iter_no_change`, we stop.</span></span><br><span class="line">    mod.fit(X, y)</span><br><span class="line">    <span class="keyword">return</span> mod</span><br></pre></td></tr></table></figure>
<p>A noteworthy feature of this <code>fit_nn_classifier</code> is that it sets <code>early_stopping=True</code>. This instructs the optimizer to hold out a small fraction (see <code>validation_fraction</code>) of the training data to use as a dev set at the end of each epoch. Optimization will stop if improvements of at least <code>tol</code> on this dev set aren’t seen within <code>n_iter_no_change</code> epochs. If that condition is triggered, the parameters from the top-scoring model are used for the final model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_ = experiment(</span><br><span class="line">    train_reader(SST_HOME),</span><br><span class="line">    unigrams_phi,</span><br><span class="line">    fit_nn_classifier)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509200355049.png" class title="图片">
<h3 id="A-softmax-classifier-in-PyTorch"><a href="#A-softmax-classifier-in-PyTorch" class="headerlink" title="A softmax classifier in PyTorch"></a>A softmax classifier in PyTorch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TorchSoftmaxClassifier</span>(<span class="title class_ inherited__">TorchShallowNeuralClassifier</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_graph</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> nn.Linear(self.input_dim, self.n_classes_)</span><br></pre></td></tr></table></figure>
<p>For this function call, add an L2 regularization term to help prevent overfitting:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit_torch_softmax</span>(<span class="params">X, y</span>):</span><br><span class="line">    mod = TorchSoftmaxClassifier(l2_strength=<span class="number">0.0001</span>)</span><br><span class="line">    mod.fit(X, y)</span><br><span class="line">    <span class="keyword">return</span> mod</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_ = experiment(</span><br><span class="line">    train_reader(SST_HOME),</span><br><span class="line">    unigrams_phi,</span><br><span class="line">    fit_torch_softmax)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509200647452.png" class title="图片">
<h3 id="Using-sklearn-Pipelines"><a href="#Using-sklearn-Pipelines" class="headerlink" title="Using sklearn Pipelines"></a>Using sklearn Pipelines</h3><p>The <code>sklearn.pipeline</code> module defines <code>Pipeline</code> objects, which let you chain together different transformations and estimators. <code>Pipeline</code> objects are fully compatible with <code>experiment</code>. Here’s a basic example using <code>TfidfTransformer</code> followed by <code>LogisticRegression</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit_pipeline_softmax</span>(<span class="params">X, y</span>):</span><br><span class="line">    rescaler = TfidfTransformer()</span><br><span class="line">    mod = LogisticRegression(max_iter=<span class="number">2000</span>)</span><br><span class="line">    pipeline = Pipeline([</span><br><span class="line">        (<span class="string">&#x27;scaler&#x27;</span>, rescaler),</span><br><span class="line">        (<span class="string">&#x27;model&#x27;</span>, mod)])</span><br><span class="line">    pipeline.fit(X, y)</span><br><span class="line">    <span class="keyword">return</span> pipeline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_ = experiment(</span><br><span class="line">    train_reader(SST_HOME),</span><br><span class="line">    unigrams_phi,</span><br><span class="line">    fit_pipeline_softmax)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509201108536.png" class title="图片">
<p>The one gotcha here is that some <code>sklearn</code> transformers return sparse matrices, which are likely to clash with the requirements of these other models. To get around this, just add <code>DenseTransformer()</code> where you need to transition from a sparse matrix to a dense one. Here’s an example using <code>TorchShallowNeuralClassifier</code> with early stopping:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DenseTransformer</span>(<span class="title class_ inherited__">TransformerMixin</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    From</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Some sklearn methods return sparse matrices that don&#x27;t interact</span></span><br><span class="line"><span class="string">    well with estimators that expect dense arrays or regular iterables</span></span><br><span class="line"><span class="string">    as inputs. This little class helps manage that. Especially useful</span></span><br><span class="line"><span class="string">    in the context of Pipelines.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y=<span class="literal">None</span>, **fit_params</span>):</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transform</span>(<span class="params">self, X, y=<span class="literal">None</span>, **fit_params</span>):</span><br><span class="line">        <span class="keyword">return</span> X.todense()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit_transform</span>(<span class="params">self, X, y=<span class="literal">None</span>, **fit_params</span>):</span><br><span class="line">        self.fit(X, y, **fit_params)</span><br><span class="line">        <span class="keyword">return</span> self.transform(X)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit_pipeline_classifier</span>(<span class="params">X, y</span>):</span><br><span class="line">    rescaler = TfidfTransformer()</span><br><span class="line">    mod = TorchShallowNeuralClassifier(early_stopping=<span class="literal">True</span>)</span><br><span class="line">    pipeline = Pipeline([</span><br><span class="line">        (<span class="string">&#x27;scaler&#x27;</span>, rescaler),</span><br><span class="line">        <span class="comment"># We need this little bridge to go from</span></span><br><span class="line">        <span class="comment"># sparse matrices to dense ones:</span></span><br><span class="line">        (<span class="string">&#x27;densify&#x27;</span>, utils.DenseTransformer()),</span><br><span class="line">        (<span class="string">&#x27;model&#x27;</span>, mod)])</span><br><span class="line">    pipeline.fit(X, y)</span><br><span class="line">    <span class="keyword">return</span> pipeline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_ = experiment(</span><br><span class="line">    train_reader(SST_HOME),</span><br><span class="line">    unigrams_phi,</span><br><span class="line">    fit_pipeline_classifier)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509201807737.png" class title="图片">
<h2 id="Hyperparameter-search"><a href="#Hyperparameter-search" class="headerlink" title="Hyperparameter search"></a>Hyperparameter search</h2><p>训练过程学习参数——权重。通常需要设置很多其他参数。例如，我们的<code>BasicSGDClassifier</code>有一个学习率参数和一个训练迭代参数。这些被称为超参数。更强大的<code>sklearn</code>分类器和我们的<code>torch_*</code>模型有更多这样的超参数。这些都超出了明确规定的目标，因此是“超”部分。</p>
<p>到目前为止，我们只是手动设置超参数。然而，它们的最佳值在不同的数据集之间可能有很大差异，这里的选择可能会极大地影响性能，因此我们希望将它们设置为整个实验框架的一部分。</p>
<h3 id="fit-classifier-with-hyperparameter-search"><a href="#fit-classifier-with-hyperparameter-search" class="headerlink" title="fit_classifier_with_hyperparameter_search"></a>fit_classifier_with_hyperparameter_search</h3><p>Luckily, <code>sklearn</code> provides a lot of functionality for setting hyperparameters via cross-validation. The function <code>fit_classifier_with_hyperparameter_search</code> implements a basic framework for taking advantage of these options. It’s really just a lightweight wrapper around <code>slearn.model_selection.GridSearchCV</code>.</p>
<p>This corresponding model wrappers have the same basic shape as <code>fit_softmax_classifier</code> above: they take a dataset as input and return a trained model.  However, to find the best model, they explore a space of hyperparameters supplied by the user, seeking the optimal combination of settings. </p>
<p>Only the training data is used to perform this search; that data is  split into multiple train–test splits, and the best hyperparameter  settings are the one that do the best on average across these splits.  Once those settings are found, a model is trained with those settings on all the available data and finally evaluated on the assessment data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV, StratifiedShuffleSplit</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit_classifier_with_hyperparameter_search</span>(<span class="params"></span></span><br><span class="line"><span class="params">        X, y, basemod, cv, param_grid, scoring=<span class="string">&#x27;f1_macro&#x27;</span>, verbose=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Fit a classifier with hyperparameters set via cross-validation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X : 2d np.array</span></span><br><span class="line"><span class="string">        The matrix of features, one example per row.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    y : list</span></span><br><span class="line"><span class="string">        The list of labels for rows in `X`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    basemod : an sklearn model class instance</span></span><br><span class="line"><span class="string">        This is the basic model-type we&#x27;ll be optimizing.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    cv : int or an sklearn Splitter</span></span><br><span class="line"><span class="string">        Number of cross-validation folds, or the object used to define</span></span><br><span class="line"><span class="string">        the splits. For example, where there is a predefeined train/dev</span></span><br><span class="line"><span class="string">        split one wants to use, one can feed in a `PredefinedSplitter`</span></span><br><span class="line"><span class="string">        instance to use that split during cross-validation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    param_grid : dict</span></span><br><span class="line"><span class="string">        A dict whose keys name appropriate parameters for `basemod` and</span></span><br><span class="line"><span class="string">        whose values are lists of values to try.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    scoring : value to optimize for (default: f1_macro)</span></span><br><span class="line"><span class="string">        Other options include &#x27;accuracy&#x27; and &#x27;f1_micro&#x27;. See</span></span><br><span class="line"><span class="string">        http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    verbose : bool</span></span><br><span class="line"><span class="string">        Whether to print some summary information to standard output.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Prints</span></span><br><span class="line"><span class="string">    ------</span></span><br><span class="line"><span class="string">    To standard output (if `verbose=True`)</span></span><br><span class="line"><span class="string">        The best parameters found.</span></span><br><span class="line"><span class="string">        The best macro F1 score obtained.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    An instance of the same class as `basemod`.</span></span><br><span class="line"><span class="string">        A trained model instance, the best model found.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(cv, <span class="built_in">int</span>):</span><br><span class="line">        cv = StratifiedShuffleSplit(n_splits=cv, test_size=<span class="number">0.20</span>)</span><br><span class="line">    <span class="comment"># Find the best model within param_grid:</span></span><br><span class="line">    crossvalidator = GridSearchCV(basemod, param_grid, cv=cv, scoring=scoring)</span><br><span class="line">    crossvalidator.fit(X, y)</span><br><span class="line">    <span class="comment"># Report some information:</span></span><br><span class="line">    <span class="keyword">if</span> verbose:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Best params: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(crossvalidator.best_params_))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Best score: &#123;0:0.03f&#125;&quot;</span>.<span class="built_in">format</span>(crossvalidator.best_score_))</span><br><span class="line">    <span class="comment"># Return the best model found:</span></span><br><span class="line">    <span class="keyword">return</span> crossvalidator.best_estimator_</span><br></pre></td></tr></table></figure>
<h3 id="Example-using-LogisticRegression"><a href="#Example-using-LogisticRegression" class="headerlink" title="Example using LogisticRegression"></a>Example using LogisticRegression</h3><p>Here’s a fairly full-featured use of the above for the <code>LogisticRegression</code> model family:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit_softmax_with_hyperparameter_search</span>(<span class="params">X, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A MaxEnt model of dataset with hyperparameter cross-validation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Some notes:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    * &#x27;fit_intercept&#x27;: whether to include the class bias feature.</span></span><br><span class="line"><span class="string">    * &#x27;C&#x27;: weight for the regularization term (smaller is more regularized).</span></span><br><span class="line"><span class="string">    * &#x27;penalty&#x27;: type of regularization -- roughly, &#x27;l1&#x27; ecourages small</span></span><br><span class="line"><span class="string">      sparse models, and &#x27;l2&#x27; encourages the weights to conform to a</span></span><br><span class="line"><span class="string">      gaussian prior distribution.</span></span><br><span class="line"><span class="string">    * &#x27;class_weight&#x27;: &#x27;balanced&#x27; adjusts the weights to simulate a</span></span><br><span class="line"><span class="string">      balanced class distribution, whereas None makes no adjustment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Other arguments can be cross-validated; see</span></span><br><span class="line"><span class="string">    http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X : 2d np.array</span></span><br><span class="line"><span class="string">        The matrix of features, one example per row.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    y : list</span></span><br><span class="line"><span class="string">        The list of labels for rows in `X`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    sklearn.linear_model.LogisticRegression</span></span><br><span class="line"><span class="string">        A trained model instance, the best model found.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    basemod = LogisticRegression(</span><br><span class="line">        fit_intercept=<span class="literal">True</span>,</span><br><span class="line">        solver=<span class="string">&#x27;liblinear&#x27;</span>,</span><br><span class="line">        multi_class=<span class="string">&#x27;auto&#x27;</span>)</span><br><span class="line">    cv = <span class="number">5</span></span><br><span class="line">    param_grid = &#123;</span><br><span class="line">        <span class="string">&#x27;C&#x27;</span>: [<span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1.0</span>, <span class="number">2.0</span>],</span><br><span class="line">        <span class="string">&#x27;penalty&#x27;</span>: [<span class="string">&#x27;l1&#x27;</span>, <span class="string">&#x27;l2&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;class_weight&#x27;</span>: [<span class="string">&#x27;balanced&#x27;</span>, <span class="literal">None</span>]&#125;</span><br><span class="line">    bestmod = fit_classifier_with_hyperparameter_search(</span><br><span class="line">        X, y, basemod, cv, param_grid)</span><br><span class="line">    <span class="keyword">return</span> bestmod</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">softmax_experiment = experiment(</span><br><span class="line">    train_reader(SST_HOME),</span><br><span class="line">    unigrams_phi,</span><br><span class="line">    fit_softmax_with_hyperparameter_search,</span><br><span class="line">    assess_dataframes=dev_reader(SST_HOME))</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509202448684.png" class title="图片">
<p>Recall that the “Best params” are found via evaluations only on the training data. The <code>assess_reader</code> is held out from that process, so it’s giving us an estimate of how we will do on a final test set.</p>
<h2 id="Reproducing-baselines-from-Socher-et-al-2013"><a href="#Reproducing-baselines-from-Socher-et-al-2013" class="headerlink" title="Reproducing  baselines from Socher et al. 2013"></a>Reproducing  baselines from Socher et al. 2013</h2><p>The goal of this section is to bring together ideas from the above to reproduce some of the the non-neural baselines from <a target="_blank" rel="noopener" href="http://www.aclweb.org/anthology/D/D13/D13-1170.pdf">Socher et al., Table 1</a>. More specifically, we’ll shoot for the root-level binary numbers:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Unigram NaiveBayes</td>
<td>81.8</td>
</tr>
<tr>
<td>Bigram NaiveBayes</td>
<td>83.1</td>
</tr>
<tr>
<td>SVM</td>
<td>79.4</td>
</tr>
</tbody>
</table>
</div>
<p>The following reduces the dataset to the binary task:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">train_df = train_reader(SST_HOME)</span><br><span class="line"></span><br><span class="line">train_bin_df = train_df[train_df.label != <span class="string">&#x27;neutral&#x27;</span>]</span><br><span class="line"></span><br><span class="line">dev_df = dev_reader(SST_HOME)</span><br><span class="line"></span><br><span class="line">dev_bin_df = dev_df[dev_df.label != <span class="string">&#x27;neutral&#x27;</span>]</span><br><span class="line"></span><br><span class="line">test_df = sentiment_reader(os.path.join(SST_HOME, <span class="string">&quot;sst3-test-labeled.csv&quot;</span>))</span><br><span class="line"></span><br><span class="line">test_bin_df = test_df[test_df.label != <span class="string">&#x27;neutral&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>Note: we will continue to train on just the full examples, so that the  experiments do not require a lot of time and computational resources.  However, there are probably gains to be had from training on the  subtrees as well. In that case, one needs to be careful in cross-validation: the test set needs to be the root-only dev set rather  than slices of the train set. To achieve this, one can use a <code>PredefinedSplit</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> PredefinedSplit</span><br><span class="line"></span><br><span class="line">full_train_df = train_reader(SST_HOME, include_subtrees=<span class="literal">True</span>)</span><br><span class="line">full_train_bin_df = full_train_df[full_train_df.label != <span class="string">&#x27;neutral&#x27;</span>]</span><br><span class="line"></span><br><span class="line">split_indices = [<span class="number">0</span>] * full_train_bin_df.shape[<span class="number">0</span>]</span><br><span class="line">split_indices += [-<span class="number">1</span>] * dev_bin_df.shape[<span class="number">0</span>]</span><br><span class="line">sst_train_dev_splitter = PredefinedSplit(split_indices)</span><br></pre></td></tr></table></figure>
<p>This would be used in place of <code>cv=5</code> in the model wrappers below.</p>
<h3 id="Reproducing-the-Unigram-NaiveBayes-results"><a href="#Reproducing-the-Unigram-NaiveBayes-results" class="headerlink" title="Reproducing the Unigram NaiveBayes results"></a>Reproducing the Unigram NaiveBayes results</h3><p>To start, we might just use <code>MultinomialNB</code> with default parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit_unigram_nb_classifier</span>(<span class="params">X, y</span>):</span><br><span class="line">    mod = MultinomialNB()</span><br><span class="line">    mod.fit(X, y)</span><br><span class="line">    <span class="keyword">return</span> mod</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">_ = experiment(</span><br><span class="line">    train_bin_df,</span><br><span class="line">    unigrams_phi,</span><br><span class="line">    fit_unigram_nb_classifier,</span><br><span class="line">    assess_dataframes=dev_bin_df)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509203252343.png" class title="图片">
<p>This falls slightly short of our goal, which is not encouraging about how we would do on the test set. However, MultinomialNB has a regularization term alpha that might have a significant impact given the very large, sparse feature matrices we are creating with unigrams_phi. In addition, it might help to transform the raw feature counts, for the same reason that reweighting was so powerful in our VSM module. The best way to try out all these ideas is to do a wide hyperparameter search. The following model wrapper function implements these steps: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit_nb_classifier_with_hyperparameter_search</span>(<span class="params">X, y</span>):</span><br><span class="line">    rescaler = TfidfTransformer()</span><br><span class="line">    mod = MultinomialNB()</span><br><span class="line"></span><br><span class="line">    pipeline = Pipeline([(<span class="string">&#x27;scaler&#x27;</span>, rescaler), (<span class="string">&#x27;model&#x27;</span>, mod)])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Access the alpha and fit_prior parameters of `mod` with</span></span><br><span class="line">    <span class="comment"># `model__alpha` and `model__fit_prior`, where &quot;model&quot; is the</span></span><br><span class="line">    <span class="comment"># name from the Pipeline. Use &#x27;passthrough&#x27; to optionally</span></span><br><span class="line">    <span class="comment"># skip TF-IDF.</span></span><br><span class="line">    param_grid = &#123;</span><br><span class="line">        <span class="string">&#x27;model__fit_prior&#x27;</span>: [<span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">        <span class="string">&#x27;scaler&#x27;</span>: [<span class="string">&#x27;passthrough&#x27;</span>, rescaler],</span><br><span class="line">        <span class="string">&#x27;model__alpha&#x27;</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.8</span>, <span class="number">1.0</span>, <span class="number">1.2</span>]&#125;</span><br><span class="line"></span><br><span class="line">    bestmod = utils.fit_classifier_with_hyperparameter_search(</span><br><span class="line">        X, y, pipeline,</span><br><span class="line">        param_grid=param_grid,</span><br><span class="line">        cv=<span class="number">5</span>)</span><br><span class="line">    <span class="keyword">return</span> bestmod</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">unigram_nb_experiment_xval = experiment(</span><br><span class="line">    [train_bin_df, dev_bin_df],</span><br><span class="line">    unigrams_phi,</span><br><span class="line">    fit_nb_classifier_with_hyperparameter_search,</span><br><span class="line">    assess_dataframes=test_bin_df)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509203421550.png" class title="图片">
<p>We’re above the target of 81.8, so we can say that we reproduced the paper’s result.</p>
<h3 id="Reproducing-the-Bigrams-NaiveBayes-results"><a href="#Reproducing-the-Bigrams-NaiveBayes-results" class="headerlink" title="Reproducing the Bigrams NaiveBayes results"></a>Reproducing the Bigrams NaiveBayes results</h3><p>For the bigram NaiveBayes mode, we can continue to use <code>fit_nb_classifier_with_hyperparameter_search</code>, but now the experiment is done with <code>bigrams_phi</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bigram_nb_experiment_xval = experiment(</span><br><span class="line">    [train_bin_df, dev_bin_df],</span><br><span class="line">    bigrams_phi,</span><br><span class="line">    fit_nb_classifier_with_hyperparameter_search,</span><br><span class="line">    assess_dataframes=test_bin_df)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509203622089.png" class title="图片">
<p>This is below the target of 83.1, so we’ve failed to reproduce the paper’s  result.</p>
<h3 id="Reproducing-the-SVM-results"><a href="#Reproducing-the-SVM-results" class="headerlink" title="Reproducing the SVM results"></a>Reproducing the SVM results</h3><p>def fit_svm_classifier_with_hyperparameter_search(X, y):<br>    rescaler = TfidfTransformer()<br>    mod = LinearSVC(loss=’squared_hinge’, penalty=’l2’)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">pipeline = Pipeline([(<span class="string">&#x27;scaler&#x27;</span>, rescaler), (<span class="string">&#x27;model&#x27;</span>, mod)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Access the alpha parameter of `mod` with `mod__alpha`,</span></span><br><span class="line"><span class="comment"># where &quot;model&quot; is the name from the Pipeline. Use</span></span><br><span class="line"><span class="comment"># &#x27;passthrough&#x27; to optionally skip TF-IDF.</span></span><br><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">&#x27;scaler&#x27;</span>: [<span class="string">&#x27;passthrough&#x27;</span>, rescaler],</span><br><span class="line">    <span class="string">&#x27;model__C&#x27;</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1.0</span>, <span class="number">1.2</span>, <span class="number">1.4</span>]&#125;</span><br><span class="line"></span><br><span class="line">bestmod = fit_classifier_with_hyperparameter_search(</span><br><span class="line">    X, y, pipeline,</span><br><span class="line">    param_grid=param_grid,</span><br><span class="line">    cv=<span class="number">5</span>)</span><br><span class="line"><span class="keyword">return</span> bestmod</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">svm_experiment_xval = experiment(</span><br><span class="line">    [train_bin_df, dev_bin_df],</span><br><span class="line">    unigrams_phi,</span><br><span class="line">    fit_svm_classifier_with_hyperparameter_search,</span><br><span class="line">    assess_dataframes=test_bin_df)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509203757079.png" class title="图片">
<p>Right on! This is quite a ways above the target of 79.4, so we can say that we successfully reproduced this result.</p>
<h2 id="Statistical-comparison-of-classifier-models"><a href="#Statistical-comparison-of-classifier-models" class="headerlink" title="Statistical comparison of classifier models"></a>Statistical comparison of classifier models</h2><p>Suppose two classifiers differ according to an effectiveness measure like F1 or accuracy. Are they meaningfully different?</p>
<ul>
<li>For very large datasets, the answer might be clear: if  performance is very stable across different train/assess splits and the  difference in terms of correct predictions has practical importance,  then you can clearly say yes. </li>
<li>With smaller datasets, or models whose performance is closer  together, it can be harder to determine whether the two models are  different. We can address this question in a basic way with repeated  runs and basic null-hypothesis testing on the resulting score vectors.</li>
</ul>
<p>In general, one wants to compare <strong>two feature functions against the same model</strong>, or one wants to compare <strong>two models with the same feature function used for both</strong>. If both are changed at the same time, then it will be hard to figure out what is causing any differences you see.</p>
<h3 id="Comparison-with-the-Wilcoxon-signed-rank-test"><a href="#Comparison-with-the-Wilcoxon-signed-rank-test" class="headerlink" title="Comparison with the Wilcoxon signed-rank test"></a>Comparison with the Wilcoxon signed-rank test</h3><p>The function <code>compare_models</code> is designed for such testing. The default set-up uses the non-parametric <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test">Wilcoxon signed-rank test</a> to make the comparisons, which is relatively conservative and recommended by <a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/v7/demsar06a.html">Demšar 2006</a> for cases where one can afford to do multiple assessments. For discussion, see <a href="evaluation_methods.ipynb#Wilcoxon-signed-rank-test">the evaluation methods notebook</a>.</p>
<p>Here’s an example showing the default parameters values and comparing <code>LogisticRegression</code> and <code>BasicSGDClassifier</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compare_models</span>(<span class="params"></span></span><br><span class="line"><span class="params">        dataframes,</span></span><br><span class="line"><span class="params">        phi1,</span></span><br><span class="line"><span class="params">        train_func1,</span></span><br><span class="line"><span class="params">        phi2=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        train_func2=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        vectorize1=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        vectorize2=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        stats_test=scipy.stats.wilcoxon,</span></span><br><span class="line"><span class="params">        trials=<span class="number">10</span>,</span></span><br><span class="line"><span class="params">        train_size=<span class="number">0.7</span>,</span></span><br><span class="line"><span class="params">        score_func=utils.safe_macro_f1</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Wrapper for comparing models. The parameters are like those of</span></span><br><span class="line"><span class="string">    `experiment`, with the same defaults, except</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    dataframes : pd.DataFrame or list of pd.DataFrame</span></span><br><span class="line"><span class="string">        The dataset or datasets to process, as read in by</span></span><br><span class="line"><span class="string">        `sentiment_reader`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    phi1, phi2</span></span><br><span class="line"><span class="string">        Just like `phi` for `experiment`. `phi1` defaults to</span></span><br><span class="line"><span class="string">        `unigrams_phi`. If `phi2` is None, then it is set equal</span></span><br><span class="line"><span class="string">        to `phi1`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    train_func1, train_func2</span></span><br><span class="line"><span class="string">        Just like `train_func` for `experiment`. If `train_func2`</span></span><br><span class="line"><span class="string">        is None, then it is set equal to `train_func`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    vectorize1, vectorize1 : bool</span></span><br><span class="line"><span class="string">        Whether to vectorize the respective inputs. Use `False` for</span></span><br><span class="line"><span class="string">        deep learning models that featurize their own input.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    stats_test : scipy.stats function</span></span><br><span class="line"><span class="string">        Defaults to `scipy.stats.wilcoxon`, a non-parametric version</span></span><br><span class="line"><span class="string">        of the paired t-test.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    trials : int (default: 10)</span></span><br><span class="line"><span class="string">        Number of runs on random train/test splits of `reader`,</span></span><br><span class="line"><span class="string">        with `train_size` controlling the amount of training data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    train_size : float</span></span><br><span class="line"><span class="string">        Percentage of data to use for training.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Prints</span></span><br><span class="line"><span class="string">    ------</span></span><br><span class="line"><span class="string">    To standard output</span></span><br><span class="line"><span class="string">        A report of the assessment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    (np.array, np.array, float)</span></span><br><span class="line"><span class="string">        The first two are the scores from each model (length `trials`),</span></span><br><span class="line"><span class="string">        and the third is the p-value returned by `stats_test`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> phi2 == <span class="literal">None</span>:</span><br><span class="line">        phi2 = phi1</span><br><span class="line">    <span class="keyword">if</span> train_func2 == <span class="literal">None</span>:</span><br><span class="line">        train_func2 = train_func1</span><br><span class="line">    experiments1 = [experiment(dataframes,</span><br><span class="line">        phi=phi1,</span><br><span class="line">        train_func=train_func1,</span><br><span class="line">        score_func=score_func,</span><br><span class="line">        vectorize=vectorize1,</span><br><span class="line">        verbose=<span class="literal">False</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(trials)]</span><br><span class="line">    experiments2 = [experiment(dataframes,</span><br><span class="line">        phi=phi2,</span><br><span class="line">        train_func=train_func2,</span><br><span class="line">        score_func=score_func,</span><br><span class="line">        vectorize=vectorize2,</span><br><span class="line">        verbose=<span class="literal">False</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(trials)]</span><br><span class="line">    scores1 = np.array([d[<span class="string">&#x27;scores&#x27;</span>][<span class="number">0</span>] <span class="keyword">for</span> d <span class="keyword">in</span> experiments1])</span><br><span class="line">    scores2 = np.array([d[<span class="string">&#x27;scores&#x27;</span>][<span class="number">0</span>] <span class="keyword">for</span> d <span class="keyword">in</span> experiments2])</span><br><span class="line">    <span class="comment"># stats_test returns (test_statistic, p-value). We keep just the p-value:</span></span><br><span class="line">    pval = stats_test(scores1, scores2)[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># Report:</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Model 1 mean: &#123;0:.03f&#125;&#x27;</span>.<span class="built_in">format</span>(scores1.mean()))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Model 2 mean: &#123;0:.03f&#125;&#x27;</span>.<span class="built_in">format</span>(scores2.mean()))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;p = &#123;0:.03f&#125;&#x27;</span>.<span class="built_in">format</span>(pval <span class="keyword">if</span> pval &gt;= <span class="number">0.001</span> <span class="keyword">else</span> <span class="string">&#x27;p &lt; 0.001&#x27;</span>))</span><br><span class="line">    <span class="comment"># Return the scores for later analysis, and the p value:</span></span><br><span class="line">    <span class="keyword">return</span> scores1, scores2, pval</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">_ = compare_models(</span><br><span class="line">    train_reader(SST_HOME),</span><br><span class="line">    unigrams_phi,</span><br><span class="line">    fit_softmax_classifier,</span><br><span class="line">    stats_test=scipy.stats.wilcoxon,</span><br><span class="line">    trials=<span class="number">10</span>,</span><br><span class="line">    phi2=<span class="literal">None</span>,  <span class="comment"># Defaults to same as first argument.</span></span><br><span class="line">    train_func2=fit_basic_sgd_classifier, <span class="comment"># Defaults to same as second argument.</span></span><br><span class="line">    train_size=<span class="number">0.7</span>,</span><br><span class="line">    score_func=utils.safe_macro_f1)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220509204233699.png" class title="图片">
<h3 id="Comparison-with-McNemar’s-test"><a href="#Comparison-with-McNemar’s-test" class="headerlink" title="Comparison with McNemar’s test"></a>Comparison with McNemar’s test</h3><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/McNemar%27s_test">McNemar’s test</a> operates directly on the vectors of predictions for the two models being compared. As such, it doesn’t require repeated runs, which is good where optimization is expensive. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mcnemar</span>(<span class="params">y_true, pred_a, pred_b</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    McNemar&#x27;s test using the chi2 distribution.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    y_true : list of actual labels</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    pred_a, pred_b : lists</span></span><br><span class="line"><span class="string">        Predictions from the two systems being evaluated.</span></span><br><span class="line"><span class="string">        Assumed to have the same length as `y_true`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    float, float (the test statistic and p value)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    c01 = <span class="number">0</span></span><br><span class="line">    c10 = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> y, a, b <span class="keyword">in</span> <span class="built_in">zip</span>(y_true, pred_a, pred_b):</span><br><span class="line">        <span class="keyword">if</span> a == y <span class="keyword">and</span> b != y:</span><br><span class="line">            c01 += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> a != y <span class="keyword">and</span> b == y:</span><br><span class="line">            c10 += <span class="number">1</span></span><br><span class="line">    stat = ((np.<span class="built_in">abs</span>(c10 - c01) - <span class="number">1.0</span>)**<span class="number">2</span>) / (c10 + c01)</span><br><span class="line">    df = <span class="number">1</span></span><br><span class="line">    pval = stats.chi2.sf(stat, df)</span><br><span class="line">    <span class="keyword">return</span> stat, pval</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m = mcnemar(</span><br><span class="line">    unigram_nb_experiment_xval[<span class="string">&#x27;assess_datasets&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;y&#x27;</span>],</span><br><span class="line">    unigram_nb_experiment_xval[<span class="string">&#x27;predictions&#x27;</span>][<span class="number">0</span>],</span><br><span class="line">    bigram_nb_experiment_xval[<span class="string">&#x27;predictions&#x27;</span>][<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p = <span class="string">&quot;p &lt; 0.0001&quot;</span> <span class="keyword">if</span> m[<span class="number">1</span>] &lt; <span class="number">0.0001</span> <span class="keyword">else</span> m[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;McNemar&#x27;s test: &#123;0:0.02f&#125; (&#123;1:&#125;)&quot;</span>.<span class="built_in">format</span>(m[<span class="number">0</span>], p))</span><br><span class="line"><span class="comment"># out: McNemar&#x27;s test: 22.38 (p &lt; 0.0001)</span></span><br></pre></td></tr></table></figure>
<h1 id="Feature-representation"><a href="#Feature-representation" class="headerlink" title="Feature representation"></a>Feature representation</h1><h2 id="N-gram-feature-functions"><a href="#N-gram-feature-functions" class="headerlink" title="N-gram feature functions"></a>N-gram feature functions</h2><ul>
<li>Unigrams: the basis for “bag-of-words” models</li>
<li>Easily generalized to “bag of-ngrams”</li>
<li>Highly dependent on the tokenization scheme</li>
<li>Can be combined with preprocessing steps like ‘_NEG’ marking</li>
<li>Creates very large, very sparse feature representations</li>
<li>Generally fails to directly model relationships between features</li>
</ul>
<h2 id="Other-ideas-for-hand-built-feature-functions"><a href="#Other-ideas-for-hand-built-feature-functions" class="headerlink" title="Other ideas for hand-built feature functions"></a>Other ideas for hand-built feature functions</h2><ul>
<li><p>Lexicon-derived features</p>
</li>
<li><p>Negation marking</p>
</li>
<li><p>Modal adverbs:</p>
<ul>
<li><p>“It is quite possibly a masterpiece.”</p>
</li>
<li><p>“It is totally amazing.”</p>
</li>
</ul>
</li>
<li><p>Length based features</p>
</li>
<li><p>Thwarted expectations: ratio of positive to negative words</p>
<ul>
<li><p>“Many consider the movie bewildering, boring, slow-moving or annoying.”</p>
</li>
<li><p>“It was hailed as a brilliant, unprecedented artistic achievement worthy of multiple Oscars.”</p>
</li>
</ul>
</li>
<li><p>Non-literal language:</p>
<ul>
<li><p>“Not exactly a masterpiece.”</p>
</li>
<li><p>“Like 50 hours long.”</p>
</li>
<li><p>“The best movie in the history of the universe.”</p>
</li>
</ul>
</li>
</ul>
<h2 id="Assessing-individual-feature-functions"><a href="#Assessing-individual-feature-functions" class="headerlink" title="Assessing individual feature functions"></a>Assessing individual feature functions</h2><ol>
<li><p><code>sklearn.feature_selection</code> offers functions to assess how much information your feature functions contain with respect to your labels.</p>
</li>
<li><p>Take care when assessing feature functions individually; correlations between them will make these assessments hard to interpret:</p>
</li>
<li><p>Consider more holistic assessment methods: systematically removing or disrupting features in the context of a full model and comparing performance before and after</p>
</li>
</ol>
<img src="/2022/05/11/SST/image-20220509205801214.png" class title="图片">
<p>What do the scores tell us about the best model? In truth, a linear model performs best with just <em>X</em>1, and including <em>X</em>2 hurts.</p>
<h2 id="Distributed-representations-as-features"><a href="#Distributed-representations-as-features" class="headerlink" title="Distributed representations as features"></a>Distributed representations as features</h2><img src="/2022/05/11/SST/image-20220509205852122.png" class title="图片">
<h1 id="RNN-classifiers"><a href="#RNN-classifiers" class="headerlink" title="RNN classifiers"></a>RNN classifiers</h1><h2 id="Model-overview"><a href="#Model-overview" class="headerlink" title="Model overview"></a>Model overview</h2><img src="/2022/05/11/SST/image-20220509210022143.png" class title="图片">
<img src="/2022/05/11/SST/image-20220509210646272.png" class title="图片">
<h2 id="A-note-on-LSTMs"><a href="#A-note-on-LSTMs" class="headerlink" title="A note on LSTMs"></a>A note on LSTMs</h2><ol>
<li><p>Plain RNNs tend to perform poorly with very long sequences; as information flflows back through the network, it is lost or distorted.</p>
</li>
<li><p>LSTM cells are a prominent response to this problem: they introduce mechanisms that control the flow of information.</p>
</li>
<li><p>We won’t review all the mechanism for this here. I instead recommend these excellent blog posts, which include intuitive diagrams and discuss the motivations for the various pieces in detail:</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">Towards Data Science: Illustrated Guide to LSTM’s and GRU’s: A step by step explanation</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">colah’s blog: Understanding LSTM networks</a></p>
</li>
</ul>
</li>
</ol>
<h1 id="Dense-feature-representations-and-neural-networks"><a href="#Dense-feature-representations-and-neural-networks" class="headerlink" title="Dense feature representations and neural networks"></a>Dense feature representations and neural networks</h1><h2 id="Overview-2"><a href="#Overview-2" class="headerlink" title="Overview"></a>Overview</h2><p>This chapter defines and explores <strong>vector averaging</strong> and <strong>recurrent neural network (RNN) classifiers</strong> for the Stanford Sentiment Treebank. </p>
<p>These approaches make their predictions based on comprehensive representations of the examples: </p>
<ul>
<li>For the vector averaging models, each word is modeled, but we assume that words combine via a simple function that is insensitive to their order or constituent structure.</li>
<li>For the RNN, each word is again modeled, and we also model the sequential relationships between words.</li>
</ul>
<h2 id="Distributed-representations-as-features-1"><a href="#Distributed-representations-as-features-1" class="headerlink" title="Distributed representations as features"></a>Distributed representations as features</h2><p>As a first step in the direction of neural networks for sentiment, we can connect with our previous unit on distributed representations. Arguably, more than any specific model architecture, this is the major innovation of deep learning: <strong>rather than designing feature functions by hand, we use dense, distributed representations, often derived from unsupervised models</strong>.</p>
<img src="/2022/05/11/SST/distreps-as-features.png" class title="图片">
<p>Our model will just be <code>LogisticRegression</code>, and we’ll continue with the experiment framework from the previous notebook. Here is <code>fit_softmax_classifier</code> again:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit_softmax_classifier</span>(<span class="params">X, y</span>):</span><br><span class="line">    mod = LogisticRegression(</span><br><span class="line">        fit_intercept=<span class="literal">True</span>,</span><br><span class="line">        solver=<span class="string">&#x27;liblinear&#x27;</span>,</span><br><span class="line">        multi_class=<span class="string">&#x27;auto&#x27;</span>)</span><br><span class="line">    mod.fit(X, y)</span><br><span class="line">    <span class="keyword">return</span> mod</span><br></pre></td></tr></table></figure>
<h3 id="GloVe-inputs"><a href="#GloVe-inputs" class="headerlink" title="GloVe inputs"></a>GloVe inputs</h3><p>To illustrate this process, we’ll use the general purpose GloVe representations released by the GloVe team, at 300d:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">DATA_HOME = <span class="string">&#x27;data&#x27;</span></span><br><span class="line"></span><br><span class="line">SST_HOME = os.path.join(DATA_HOME, <span class="string">&#x27;sentiment&#x27;</span>)</span><br><span class="line"></span><br><span class="line">GLOVE_HOME = os.path.join(DATA_HOME, <span class="string">&#x27;glove.6B&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">glove2dict</span>(<span class="params">src_filename</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    GloVe vectors file reader.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    src_filename : str</span></span><br><span class="line"><span class="string">        Full path to the GloVe file to be processed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    dict</span></span><br><span class="line"><span class="string">        Mapping words to their GloVe vectors as `np.array`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># This distribution has some words with spaces, so we have to</span></span><br><span class="line">    <span class="comment"># assume its dimensionality and parse out the lines specially:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;840B.300d&#x27;</span> <span class="keyword">in</span> src_filename:</span><br><span class="line">        line_parser = <span class="keyword">lambda</span> line: line.rsplit(<span class="string">&quot; &quot;</span>, <span class="number">300</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        line_parser = <span class="keyword">lambda</span> line: line.strip().split()</span><br><span class="line">    data = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(src_filename, encoding=<span class="string">&#x27;utf8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                line = <span class="built_in">next</span>(f)</span><br><span class="line">                line = line_parser(line)</span><br><span class="line">                data[line[<span class="number">0</span>]] = np.array(line[<span class="number">1</span>: ], dtype=np.float64)</span><br><span class="line">            <span class="keyword">except</span> StopIteration:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">except</span> UnicodeDecodeError:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">glove_lookup = glove2dict(</span><br><span class="line">    os.path.join(GLOVE_HOME, <span class="string">&#x27;glove.6B.300d.txt&#x27;</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vsm_phi</span>(<span class="params">text, lookup, np_func=np.mean</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Represent `text` as a combination of the vector of its words.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    text : str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    lookup : dict</span></span><br><span class="line"><span class="string">        From words to vectors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    np_func : function (default: np.sum)</span></span><br><span class="line"><span class="string">        A numpy matrix operation that can be applied columnwise,</span></span><br><span class="line"><span class="string">        like `np.mean`, `np.sum`, or `np.prod`. The requirement is that</span></span><br><span class="line"><span class="string">        the function take `axis=0` as one of its arguments (to ensure</span></span><br><span class="line"><span class="string">        columnwise combination) and that it return a vector of a</span></span><br><span class="line"><span class="string">        fixed length, no matter what the size of the text is.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    np.array, dimension `X.shape[1]`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    allvecs = np.array([lookup[w] <span class="keyword">for</span> w <span class="keyword">in</span> text.split() <span class="keyword">if</span> w <span class="keyword">in</span> lookup])</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(allvecs) == <span class="number">0</span>:</span><br><span class="line">        dim = <span class="built_in">len</span>(<span class="built_in">next</span>(<span class="built_in">iter</span>(lookup.values())))</span><br><span class="line">        feats = np.zeros(dim)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        feats = np_func(allvecs, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> feats</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">glove_phi</span>(<span class="params">text, np_func=np.mean</span>):</span><br><span class="line">    <span class="keyword">return</span> vsm_phi(text, glove_lookup, np_func=np_func)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">_ = experiment(</span><br><span class="line">    train_reader(SST_HOME),</span><br><span class="line">    glove_phi,</span><br><span class="line">    fit_softmax_classifier,</span><br><span class="line">    assess_dataframes=sst.dev_reader(SST_HOME),</span><br><span class="line">    vectorize=<span class="literal">False</span>)  <span class="comment"># Tell `experiment` that we already have our feature vectors.</span></span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220511153546648.png" class title="图片">
<h3 id="Yelp-representations"><a href="#Yelp-representations" class="headerlink" title="Yelp representations"></a>Yelp representations</h3><p>Our Yelp VSMs seems pretty well-attuned to the SST, so we might think  that they can do even better than the general-purpose GloVe inputs. Here are two quick assessments of that idea that seeks to build on ideas we  developed in the unit on VSMs.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">yelp20 = pd.read_csv(</span><br><span class="line">    os.path.join(VSMDATA_HOME, <span class="string">&#x27;yelp_window20-flat.csv.gz&#x27;</span>), index_col=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">yelp20_ppmi = pmi(yelp20, positive=<span class="literal">False</span>)</span><br><span class="line">yelp20_ppmi_svd = lsa(yelp20_ppmi, k=<span class="number">300</span>)</span><br><span class="line">yelp_lookup = <span class="built_in">dict</span>(<span class="built_in">zip</span>(yelp20_ppmi_svd.index, yelp20_ppmi_svd.values))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">yelp_phi</span>(<span class="params">text, np_func=np.mean</span>):</span><br><span class="line">    <span class="keyword">return</span> vsm_phi(text, yelp_lookup, np_func=np_func)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">_ = experiment(</span><br><span class="line">    train_reader(SST_HOME),</span><br><span class="line">    yelp_phi,</span><br><span class="line">    fit_softmax_classifier,</span><br><span class="line">    assess_dataframes=sst.dev_reader(SST_HOME),</span><br><span class="line">    vectorize=<span class="literal">False</span>)  <span class="comment"># Tell `experiment` that we already have our feature vectors.</span></span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220511154002957.png" class title="图片">
<h3 id="Remarks-on-this-approach"><a href="#Remarks-on-this-approach" class="headerlink" title="Remarks on this approach"></a>Remarks on this approach</h3><ul>
<li><p>The above models’ feature representations have only 300 dimensions. While they are struggling with the neutral category, we can probably overcome this with some additional attention to the representations and to our strategies for optimization.</p>
</li>
<li><p>We’ll return to these ideas below, when we consider <a href="#The-VecAvg-baseline-from-Socher-et-al.-2013">the VecAvg baseline from Socher et al. 2013</a>. That model also posits a simple, fixed combination function (averaging). However, it begins with randomly initialized representations and updates them as part of training.</p>
</li>
</ul>
<h2 id="RNN-classifiers-1"><a href="#RNN-classifiers-1" class="headerlink" title="RNN classifiers"></a>RNN classifiers</h2><p>A recurrent neural network (RNN) is any deep learning model that process its inputs sequentially. There are many variations on this theme. The one that we use here is an <strong>RNN classifier</strong>.</p>
<img src="/2022/05/11/SST/rnn_classifier.png" class title="图片">
<p>The version of the model that is implemented in <code>np_rnn_classifier.py</code> corresponds exactly to the above diagram. We can express it mathematically as follows:</p>
<script type="math/tex; mode=display">
\begin{align*}
h_{t} &= \tanh(x_{t}W_{xh} + h_{t-1}W_{hh}) \\
y     &= \textbf{softmax}(h_{n}W_{hy} + b_y)
\end{align*}</script><p>where $1 \leqslant t \leqslant n$. The first line defines the recurrence: each hidden state $h_{t}$ is defined by the input $x_{t}$ and the previous hidden state $h_{t-1}$, together with weight matrices $W_{xh}$ and $W_{hh}$, which are used at all timesteps. As indicated in the above diagram, the sequence of hidden states is padded with an initial state $h_{0}$. In our implementations, this is always an all $0$ vector, but it can be initialized in more sophisticated ways (some of which we will explore in our units on natural language inference and grounded natural language generation).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># np_rnn_classifier.py</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> np_model_base <span class="keyword">import</span> NNModelBase</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> softmax, safe_macro_f1</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">&quot;Christopher Potts&quot;</span></span><br><span class="line">__version__ = <span class="string">&quot;CS224u, Stanford, Spring 2022&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNClassifier</span>(<span class="title class_ inherited__">NNModelBase</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Simple Recurrent Neural Network for classification problems.</span></span><br><span class="line"><span class="string">    The structure of the network is as follows:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                                                  y</span></span><br><span class="line"><span class="string">                                                 /|</span></span><br><span class="line"><span class="string">                                                b | W_hy</span></span><br><span class="line"><span class="string">                                                  |</span></span><br><span class="line"><span class="string">    h_0 -- W_hh --  h_1 -- W_hh -- h_2 -- W_hh -- h_3</span></span><br><span class="line"><span class="string">                     |              |              |</span></span><br><span class="line"><span class="string">                     | W_xh         | W_xh         | W_xh</span></span><br><span class="line"><span class="string">                     |              |              |</span></span><br><span class="line"><span class="string">                    x_1            x_2            x_3</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where x_i are the inputs, h_j are the hidden units, and y is a</span></span><br><span class="line"><span class="string">    one-hot vector indicating the true label for this sequence. The</span></span><br><span class="line"><span class="string">    parameters are W_xh, W_hh, W_hy, and the bias b. The inputs x_i</span></span><br><span class="line"><span class="string">    come from a user-supplied embedding space for the vocabulary. These</span></span><br><span class="line"><span class="string">    can either be random or pretrained. The network equations in brief:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        h[t] = tanh(x[t].dot(W_xh) + h[t-1].dot(W_hh))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y = softmax(h[-1].dot(W_hy) + b)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The network will work for any kind of classification task.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    vocab : list of str</span></span><br><span class="line"><span class="string">        This should be the vocabulary. It needs to be aligned with</span></span><br><span class="line"><span class="string">        `embedding` in the sense that the ith element of vocab</span></span><br><span class="line"><span class="string">        should be represented by the ith row of `embedding`. Ignored</span></span><br><span class="line"><span class="string">        if `use_embedding=False`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    embedding : np.array or None</span></span><br><span class="line"><span class="string">        Each row represents a word in `vocab`, as described above.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    use_embedding : bool</span></span><br><span class="line"><span class="string">        If True, then incoming examples are presumed to be lists of</span></span><br><span class="line"><span class="string">        elements of the vocabulary. If False, then they are presumed</span></span><br><span class="line"><span class="string">        to be lists of vectors. In this case, the `embedding` and</span></span><br><span class="line"><span class="string">       `embed_dim` arguments are ignored, since no embedding is needed</span></span><br><span class="line"><span class="string">       and `embed_dim` is set by the nature of the incoming vectors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    embed_dim : int</span></span><br><span class="line"><span class="string">        Dimensionality for the initial embeddings. This is ignored</span></span><br><span class="line"><span class="string">        if `embedding` is not None, as a specified value there</span></span><br><span class="line"><span class="string">        determines this value. Also ignored if `use_embedding=False`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    All of the above are set as attributes. In addition, `self.embed_dim`</span></span><br><span class="line"><span class="string">    is set to the dimensionality of the input representations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">            vocab,</span></span><br><span class="line"><span class="params">            embedding=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            use_embedding=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">            embed_dim=<span class="number">50</span>,</span></span><br><span class="line"><span class="params">            **kwargs</span>):</span><br><span class="line">        self.vocab = vocab</span><br><span class="line">        self.vocab_lookup = <span class="built_in">dict</span>(<span class="built_in">zip</span>(self.vocab, <span class="built_in">range</span>(<span class="built_in">len</span>(self.vocab))))</span><br><span class="line">        self.use_embedding = use_embedding</span><br><span class="line">        self._embed_dim = embed_dim</span><br><span class="line">        <span class="keyword">if</span> self.use_embedding:</span><br><span class="line">            <span class="keyword">if</span> embedding <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                embedding = self._define_embedding_matrix(</span><br><span class="line">                    <span class="built_in">len</span>(self.vocab), embed_dim)</span><br><span class="line">            self.embedding = embedding</span><br><span class="line">            self._embed_dim = self.embedding.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.params += [<span class="string">&#x27;embedding&#x27;</span>, <span class="string">&#x27;embed_dim&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">embed_dim</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._embed_dim</span><br><span class="line"></span><br><span class="line"><span class="meta">    @embed_dim.setter</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">embed_dim</span>(<span class="params">self, value</span>):</span><br><span class="line">        self._embed_dim = value</span><br><span class="line">        self.embedding = self._define_embedding_matrix(</span><br><span class="line">            <span class="built_in">len</span>(self.vocab), value)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.use_embedding:</span><br><span class="line">            self._embed_dim = <span class="built_in">len</span>(X[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().fit(X, y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">initialize_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Attributes</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        output_dim : int</span></span><br><span class="line"><span class="string">            Set based on the length of the labels in `training_data`.</span></span><br><span class="line"><span class="string">            This happens in `self.prepare_output_data`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        W_xh : np.array</span></span><br><span class="line"><span class="string">            Dense connections between the word representations</span></span><br><span class="line"><span class="string">            and the hidden layers. Random initialization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        W_hh : np.array</span></span><br><span class="line"><span class="string">            Dense connections between the hidden representations.</span></span><br><span class="line"><span class="string">            Random initialization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        W_hy : np.array</span></span><br><span class="line"><span class="string">            Dense connections from the final hidden layer to</span></span><br><span class="line"><span class="string">            the output layer. Random initialization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        b : np.array</span></span><br><span class="line"><span class="string">            Output bias. Initialized to all 0.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.W_xh = self.weight_init(self.embed_dim, self.hidden_dim)</span><br><span class="line">        self.W_hh = self.weight_init(self.hidden_dim, self.hidden_dim)</span><br><span class="line">        self.W_hy = self.weight_init(self.hidden_dim, self.output_dim)</span><br><span class="line">        self.b = np.zeros(self.output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_propagation</span>(<span class="params">self, seq</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        seq : list</span></span><br><span class="line"><span class="string">            Variable length sequence of elements in the vocabulary.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        h : np.array</span></span><br><span class="line"><span class="string">            Each row is for a hidden representation. The first row</span></span><br><span class="line"><span class="string">            is an all-0 initial state. The others correspond to</span></span><br><span class="line"><span class="string">            the inputs in seq.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y : np.array</span></span><br><span class="line"><span class="string">            The vector of predictions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        h = np.zeros((<span class="built_in">len</span>(seq)+<span class="number">1</span>, self.hidden_dim))</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(seq)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> self.use_embedding:</span><br><span class="line">                word_rep = self.get_word_rep(seq[t-<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                word_rep = seq[t-<span class="number">1</span>]</span><br><span class="line">            h[t] = self.hidden_activation(</span><br><span class="line">                word_rep.dot(self.W_xh) + h[t-<span class="number">1</span>].dot(self.W_hh))</span><br><span class="line">        y = softmax(h[-<span class="number">1</span>].dot(self.W_hy) + self.b)</span><br><span class="line">        <span class="keyword">return</span> h, y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward_propagation</span>(<span class="params">self, h, predictions, seq, labels</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        h : np.array, shape (m, self.hidden_dim)</span></span><br><span class="line"><span class="string">            Matrix of hidden states. `m` is the shape of the current</span></span><br><span class="line"><span class="string">            example (which is allowed to vary).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        predictions : np.array, dimension `len(self.classes)`</span></span><br><span class="line"><span class="string">            Vector of predictions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        seq : list  of lists</span></span><br><span class="line"><span class="string">            The original example.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        labels : np.array, dimension `len(self.classes)`</span></span><br><span class="line"><span class="string">            One-hot vector giving the true label.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        tuple</span></span><br><span class="line"><span class="string">            The matrices of derivatives (d_W_hy, d_b, d_W_hh, d_W_xh).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Output errors:</span></span><br><span class="line">        y_err = predictions</span><br><span class="line">        y_err[np.argmax(labels)] -= <span class="number">1</span></span><br><span class="line">        h_err = y_err.dot(self.W_hy.T) * self.d_hidden_activation(h[-<span class="number">1</span>])</span><br><span class="line">        d_W_hy = np.outer(h[-<span class="number">1</span>], y_err)</span><br><span class="line">        d_b = y_err</span><br><span class="line">        <span class="comment"># For accumulating the gradients through time:</span></span><br><span class="line">        d_W_hh = np.zeros(self.W_hh.shape)</span><br><span class="line">        d_W_xh = np.zeros(self.W_xh.shape)</span><br><span class="line">        <span class="comment"># Back-prop through time; the +1 is because the 0th</span></span><br><span class="line">        <span class="comment"># hidden state is the all-0s initial state.</span></span><br><span class="line">        num_steps = <span class="built_in">len</span>(seq)+<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="number">1</span>, num_steps)):</span><br><span class="line">            d_W_hh += np.outer(h[t], h_err)</span><br><span class="line">            <span class="keyword">if</span> self.use_embedding:</span><br><span class="line">                word_rep = self.get_word_rep(seq[t-<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                 word_rep = seq[t-<span class="number">1</span>]</span><br><span class="line">            d_W_xh += np.outer(word_rep, h_err)</span><br><span class="line">            h_err = h_err.dot(self.W_hh.T) * self.d_hidden_activation(h[t])</span><br><span class="line">        <span class="keyword">return</span> (d_W_hy, d_b, d_W_hh, d_W_xh)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">self, gradients</span>):</span><br><span class="line">        d_W_hy, d_b, d_W_hh, d_W_xh = gradients</span><br><span class="line">        self.W_hy -= self.eta * d_W_hy</span><br><span class="line">        self.b -= self.eta * d_b</span><br><span class="line">        self.W_hh -= self.eta * d_W_hh</span><br><span class="line">        self.W_xh -= self.eta * d_W_xh</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        preds = self.predict(X)</span><br><span class="line">        <span class="keyword">return</span> safe_macro_f1(y, preds)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simple_example</span>():</span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">    <span class="keyword">import</span> utils</span><br><span class="line"></span><br><span class="line">    utils.fix_random_seeds()</span><br><span class="line"></span><br><span class="line">    vocab = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;$UNK&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># No b before an a</span></span><br><span class="line">    train = [</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;ab&#x27;</span>), <span class="string">&#x27;good&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;aab&#x27;</span>), <span class="string">&#x27;good&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;abb&#x27;</span>), <span class="string">&#x27;good&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;aabb&#x27;</span>), <span class="string">&#x27;good&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;ba&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;baa&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;bba&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;bbaa&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;aba&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">    test = [</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;baaa&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;abaa&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;bbaa&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;aaab&#x27;</span>), <span class="string">&#x27;good&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;aaabb&#x27;</span>), <span class="string">&#x27;good&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">    X_train, y_train = <span class="built_in">zip</span>(*train)</span><br><span class="line">    X_test, y_test = <span class="built_in">zip</span>(*test)</span><br><span class="line"></span><br><span class="line">    mod = RNNClassifier(vocab)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(mod)</span><br><span class="line"></span><br><span class="line">    mod.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    preds = mod.predict(X_test)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nPredictions:&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ex, pred, gold <span class="keyword">in</span> <span class="built_in">zip</span>(X_test, preds, y_test):</span><br><span class="line">        score = <span class="string">&quot;correct&quot;</span> <span class="keyword">if</span> pred == gold <span class="keyword">else</span> <span class="string">&quot;incorrect&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;&#123;0:&gt;6&#125; - predicted: &#123;1:&gt;4&#125;; actual: &#123;2:&gt;4&#125; - &#123;3&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">            <span class="string">&quot;&quot;</span>.join(ex), pred, gold, score))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> accuracy_score(y_test, preds)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    simple_example()</span><br></pre></td></tr></table></figure>
<p>The model in <code>torch_rnn_classifier.py</code> expands on the above and allows for more flexibility:</p>
<script type="math/tex; mode=display">
\begin{align*}
h_{t} &= \text{RNN}(x_{t}, h_{t-1}) \\
h     &= f(h_{n}W_{hh} + b_{h}) \\
y     &= \textbf{softmax}(hW_{hy} + b_y)
\end{align*}</script><p>Here, $\text{RNN}$ stands for all the parameters of the recurrent part of the model. This will depend on the choice one makes for <code>rnn_cell_class</code>; options include <code>nn.RNN</code>, <code>nn.LSTM</code>, and <code>nn.GRU</code>. In addition, the classifier part includes a hidden layer (middle row), and the user can decide on the activation funtion $f$ to use there (parameter: <code>classifier_activation</code>).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch_rnn_classifier.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">from</span> torch_model_base <span class="keyword">import</span> TorchModelBase</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">&quot;Christopher Potts&quot;</span></span><br><span class="line">__version__ = <span class="string">&quot;CS224u, Stanford, Spring 2022&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TorchRNNDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sequences, seq_lengths, y=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Dataset class for RNN classifiers. The heavy-lifting is done by</span></span><br><span class="line"><span class="string">        `collate_fn`, which handles the padding and packing necessary to</span></span><br><span class="line"><span class="string">        efficiently process variable length sequences.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        sequences : list of `torch.LongTensor`, `len(n_examples)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        seq_lengths : torch.LongTensor, shape `(n_examples, )`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y : None or torch.LongTensor, shape `(n_examples, )`</span></span><br><span class="line"><span class="string">            If None, then we are in prediction mode. Otherwise, these are</span></span><br><span class="line"><span class="string">            indices into the list of classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(sequences) == <span class="built_in">len</span>(seq_lengths)</span><br><span class="line">        self.sequences = sequences</span><br><span class="line">        self.seq_lengths = seq_lengths</span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">len</span>(sequences) == <span class="built_in">len</span>(y)</span><br><span class="line">        self.y = y</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Format a batch of examples for use in both training and prediction.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        batch : tuple of length 2 (prediction) or 3 (training)</span></span><br><span class="line"><span class="string">            The first element is the list of input sequences. The</span></span><br><span class="line"><span class="string">            second is the list of lengths for those sequences. The third,</span></span><br><span class="line"><span class="string">            where present, is the list of labels.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        X : torch.Tensor, shape `(batch_size, max_batch_length)`</span></span><br><span class="line"><span class="string">            As padded by `torch.nn.utils.rnn.pad_sequence.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        seq_lengths : torch.LongTensor, shape `(batch_size, )`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y : torch.LongTensor, shape `(batch_size, )`</span></span><br><span class="line"><span class="string">            Only for training. In the case where `y` cannot be turned into</span></span><br><span class="line"><span class="string">            a Tensor, we assume it is because it is a list of variable</span></span><br><span class="line"><span class="string">            length sequences and to use `torch.nn.utils.rnn.pad_sequence`.</span></span><br><span class="line"><span class="string">            The hope is that this will accomodate sequence prediction.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_elements = <span class="built_in">list</span>(<span class="built_in">zip</span>(*batch))</span><br><span class="line">        X = batch_elements[<span class="number">0</span>]</span><br><span class="line">        seq_lengths = batch_elements[<span class="number">1</span>]</span><br><span class="line">        X = torch.nn.utils.rnn.pad_sequence(X, batch_first=<span class="literal">True</span>)</span><br><span class="line">        seq_lengths = torch.tensor(seq_lengths)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(batch_elements) == <span class="number">3</span>:</span><br><span class="line">            y = batch_elements[<span class="number">2</span>]</span><br><span class="line">            <span class="comment"># We can try to accommodate the case where `y` is a sequence</span></span><br><span class="line">            <span class="comment"># loss with potentially different lengths by resorting to</span></span><br><span class="line">            <span class="comment"># padding if creating a tensor is not possible:</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                y = torch.tensor(y)</span><br><span class="line">            <span class="keyword">except</span> (ValueError, TypeError):</span><br><span class="line">                y = torch.nn.utils.rnn.pad_sequence(y, batch_first=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">return</span> X, seq_lengths, y</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> X, seq_lengths</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.sequences)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">if</span> self.y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> self.sequences[idx], self.seq_lengths[idx], self.y[idx]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.sequences[idx], self.seq_lengths[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TorchRNNModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">            vocab_size,</span></span><br><span class="line"><span class="params">            embed_dim=<span class="number">50</span>,</span></span><br><span class="line"><span class="params">            embedding=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            use_embedding=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">            rnn_cell_class=nn.LSTM,</span></span><br><span class="line"><span class="params">            hidden_dim=<span class="number">50</span>,</span></span><br><span class="line"><span class="params">            bidirectional=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            freeze_embedding=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Defines the core RNN computation graph. For an explanation of the</span></span><br><span class="line"><span class="string">        parameters, see `TorchRNNClassifierModel`. This class handles just</span></span><br><span class="line"><span class="string">        the RNN components of the overall classifier model.</span></span><br><span class="line"><span class="string">        `TorchRNNClassifierModel` uses the output states to create a</span></span><br><span class="line"><span class="string">        classifier.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.use_embedding = use_embedding</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.freeze_embedding = freeze_embedding</span><br><span class="line">        <span class="comment"># Graph</span></span><br><span class="line">        <span class="keyword">if</span> self.use_embedding:</span><br><span class="line">            self.embedding = self._define_embedding(</span><br><span class="line">                embedding, vocab_size, self.embed_dim, self.freeze_embedding)</span><br><span class="line">            self.embed_dim = self.embedding.embedding_dim</span><br><span class="line">        self.rnn = rnn_cell_class(</span><br><span class="line">            input_size=self.embed_dim,</span><br><span class="line">            hidden_size=hidden_dim,</span><br><span class="line">            batch_first=<span class="literal">True</span>,</span><br><span class="line">            bidirectional=bidirectional)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, seq_lengths</span>):</span><br><span class="line">        <span class="keyword">if</span> self.use_embedding:</span><br><span class="line">            X = self.embedding(X)</span><br><span class="line">        embs = torch.nn.utils.rnn.pack_padded_sequence(</span><br><span class="line">            X,</span><br><span class="line">            batch_first=<span class="literal">True</span>,</span><br><span class="line">            lengths=seq_lengths.cpu(),</span><br><span class="line">            enforce_sorted=<span class="literal">False</span>)</span><br><span class="line">        outputs, state = self.rnn(embs)</span><br><span class="line">        <span class="keyword">return</span> outputs, state</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_define_embedding</span>(<span class="params">embedding, vocab_size, embed_dim, freeze_embedding</span>):</span><br><span class="line">        <span class="keyword">if</span> embedding <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            emb = nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">            emb.weight.requires_grad = <span class="keyword">not</span> freeze_embedding</span><br><span class="line">            <span class="keyword">return</span> emb</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(embedding, np.ndarray):</span><br><span class="line">            embedding = torch.FloatTensor(embedding)</span><br><span class="line">            <span class="keyword">return</span> nn.Embedding.from_pretrained(</span><br><span class="line">                embedding, freeze=freeze_embedding)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> embedding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TorchRNNClassifierModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rnn, output_dim, classifier_activation</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Defines the core computation graph for `TorchRNNClassifier`. This</span></span><br><span class="line"><span class="string">        involves using the outputs of a `TorchRNNModel` instance to</span></span><br><span class="line"><span class="string">        build a softmax classifier:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        h[t] = rnn(x[t], h[t-1])</span></span><br><span class="line"><span class="string">        h = f(h[-1].dot(W_hy) + b_h)</span></span><br><span class="line"><span class="string">        y = softmax(hW + b_y)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        This class uses its `rnn` parameter to compute each `h[1]`, and</span></span><br><span class="line"><span class="string">        then it adds the classifier parameters that use `h[-1]` as inputs.</span></span><br><span class="line"><span class="string">        Where `bidirectional=True`, `h[-1]` is `torch.cat([h[0], h[-1])`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.rnn = rnn</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        self.hidden_dim = self.rnn.hidden_dim</span><br><span class="line">        <span class="keyword">if</span> self.rnn.bidirectional:</span><br><span class="line">            self.classifier_dim = self.hidden_dim * <span class="number">2</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.classifier_dim = self.hidden_dim</span><br><span class="line">        self.hidden_layer = nn.Linear(</span><br><span class="line">            self.classifier_dim, self.hidden_dim)</span><br><span class="line">        self.classifier_activation = classifier_activation</span><br><span class="line">        self.classifier_layer = nn.Linear(</span><br><span class="line">            self.hidden_dim, self.output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, seq_lengths</span>):</span><br><span class="line">        outputs, state = self.rnn(X, seq_lengths)</span><br><span class="line">        state = self.get_batch_final_states(state)</span><br><span class="line">        <span class="keyword">if</span> self.rnn.bidirectional:</span><br><span class="line">            state = torch.cat((state[<span class="number">0</span>], state[<span class="number">1</span>]), dim=<span class="number">1</span>)</span><br><span class="line">        h = self.classifier_activation(self.hidden_layer(state))</span><br><span class="line">        logits = self.classifier_layer(h)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_batch_final_states</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="keyword">if</span> self.rnn.rnn.__class__.__name__ == <span class="string">&#x27;LSTM&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> state[<span class="number">0</span>].squeeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> state.squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TorchRNNClassifier</span>(<span class="title class_ inherited__">TorchModelBase</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">            vocab,</span></span><br><span class="line"><span class="params">            hidden_dim=<span class="number">50</span>,</span></span><br><span class="line"><span class="params">            embedding=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            use_embedding=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">            embed_dim=<span class="number">50</span>,</span></span><br><span class="line"><span class="params">            rnn_cell_class=nn.LSTM,</span></span><br><span class="line"><span class="params">            bidirectional=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            freeze_embedding=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            classifier_activation=nn.ReLU(<span class="params"></span>),</span></span><br><span class="line"><span class="params">            **base_kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        RNN-based Recurrent Neural Network for classification problems.</span></span><br><span class="line"><span class="string">        The network will work for any kind of classification task.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        vocab : list of str</span></span><br><span class="line"><span class="string">            This should be the vocabulary. It needs to be aligned with</span></span><br><span class="line"><span class="string">            `embedding` in the sense that the ith element of vocab</span></span><br><span class="line"><span class="string">            should be represented by the ith row of `embedding`. Ignored</span></span><br><span class="line"><span class="string">            if `use_embedding=False`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        embedding : np.array or None</span></span><br><span class="line"><span class="string">            Each row represents a word in `vocab`, as described above.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        use_embedding : bool</span></span><br><span class="line"><span class="string">            If True, then incoming examples are presumed to be lists of</span></span><br><span class="line"><span class="string">            elements of the vocabulary. If False, then they are presumed</span></span><br><span class="line"><span class="string">            to be lists of vectors. In this case, the `embedding` and</span></span><br><span class="line"><span class="string">            `embed_dim` arguments are ignored, since no embedding is needed</span></span><br><span class="line"><span class="string">            and `embed_dim` is set by the nature of the incoming vectors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        embed_dim : int</span></span><br><span class="line"><span class="string">            Dimensionality for the initial embeddings. This is ignored</span></span><br><span class="line"><span class="string">            if `embedding` is not None, as a specified value there</span></span><br><span class="line"><span class="string">            determines this value. Also ignored if `use_embedding=False`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        rnn_cell_class : class for PyTorch recurrent layer</span></span><br><span class="line"><span class="string">            Should be just the class name, not an instance of the class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        hidden_dim : int</span></span><br><span class="line"><span class="string">            Dimensionality of the hidden layer in the RNN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        bidirectional : bool</span></span><br><span class="line"><span class="string">            If True, then the final hidden states from passes in both</span></span><br><span class="line"><span class="string">            directions are used.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        freeze_embedding : bool</span></span><br><span class="line"><span class="string">            If True, the embedding will be updated during training. If</span></span><br><span class="line"><span class="string">            False, the embedding will be frozen. This parameter applies</span></span><br><span class="line"><span class="string">            to both randomly initialized and pretrained embeddings.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        classifier_activation : nn.Module</span></span><br><span class="line"><span class="string">            The non-activation function used by the network for the</span></span><br><span class="line"><span class="string">            hidden layer of the classifier.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        **base_kwargs</span></span><br><span class="line"><span class="string">            For details, see `torch_model_base.py`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Attributes</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        loss: nn.CrossEntropyLoss(reduction=&quot;mean&quot;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        self.params: list</span></span><br><span class="line"><span class="string">            Extends TorchModelBase.params with names for all of the</span></span><br><span class="line"><span class="string">            arguments for this class to support tuning of these values</span></span><br><span class="line"><span class="string">            using `sklearn.model_selection` tools.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.vocab = vocab</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embedding = embedding</span><br><span class="line">        self.use_embedding = use_embedding</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.rnn_cell_class = rnn_cell_class</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.freeze_embedding = freeze_embedding</span><br><span class="line">        self.classifier_activation = classifier_activation</span><br><span class="line">        <span class="built_in">super</span>().__init__(**base_kwargs)</span><br><span class="line">        self.params += [</span><br><span class="line">            <span class="string">&#x27;hidden_dim&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;embed_dim&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;embedding&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;use_embedding&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;rnn_cell_class&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;bidirectional&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;freeze_embedding&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;classifier_activation&#x27;</span>]</span><br><span class="line">        self.loss = nn.CrossEntropyLoss(reduction=<span class="string">&quot;mean&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_graph</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        The core computation graph. This is called by `fit`, which sets</span></span><br><span class="line"><span class="string">        the `self.model` attribute.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        TorchRNNModel</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        rnn = TorchRNNModel(</span><br><span class="line">            vocab_size=<span class="built_in">len</span>(self.vocab),</span><br><span class="line">            embedding=self.embedding,</span><br><span class="line">            use_embedding=self.use_embedding,</span><br><span class="line">            embed_dim=self.embed_dim,</span><br><span class="line">            rnn_cell_class=self.rnn_cell_class,</span><br><span class="line">            hidden_dim=self.hidden_dim,</span><br><span class="line">            bidirectional=self.bidirectional,</span><br><span class="line">            freeze_embedding=self.freeze_embedding)</span><br><span class="line"></span><br><span class="line">        model = TorchRNNClassifierModel(</span><br><span class="line">            rnn=rnn,</span><br><span class="line">            output_dim=self.n_classes_,</span><br><span class="line">            classifier_activation=self.classifier_activation)</span><br><span class="line"></span><br><span class="line">        self.embed_dim = rnn.embed_dim</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_dataset</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Format data for training and prediction.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : list of lists</span></span><br><span class="line"><span class="string">            The raw sequences. The lists are expected to contain</span></span><br><span class="line"><span class="string">            elements of `self.vocab`. This method converts them to</span></span><br><span class="line"><span class="string">            indices for PyTorch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y : list or None</span></span><br><span class="line"><span class="string">            The raw labels. This method turns them into indices for</span></span><br><span class="line"><span class="string">            PyTorch processing. If None, then we are in prediction</span></span><br><span class="line"><span class="string">            mode.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        TorchRNNDataset</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        X, seq_lengths = self._prepare_sequences(X)</span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> TorchRNNDataset(X, seq_lengths)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.classes_ = <span class="built_in">sorted</span>(<span class="built_in">set</span>(y))</span><br><span class="line">            self.n_classes_ = <span class="built_in">len</span>(self.classes_)</span><br><span class="line">            class2index = <span class="built_in">dict</span>(<span class="built_in">zip</span>(self.classes_, <span class="built_in">range</span>(self.n_classes_)))</span><br><span class="line">            y = [class2index[label] <span class="keyword">for</span> label <span class="keyword">in</span> y]</span><br><span class="line">            <span class="keyword">return</span> TorchRNNDataset(X, seq_lengths, y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_prepare_sequences</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Internal method for turning X into a list of indices into</span></span><br><span class="line"><span class="string">        `self.vocab` and calculating the true lengths of the elements</span></span><br><span class="line"><span class="string">        in `X`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : list of lists, `len(n_examples)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        new_X : list of lists, `len(n_examples)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        seq_lengths : torch.LongTensor, shape `(n_examples, )`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.use_embedding:</span><br><span class="line">            new_X = []</span><br><span class="line">            seq_lengths = []</span><br><span class="line">            index = <span class="built_in">dict</span>(<span class="built_in">zip</span>(self.vocab, <span class="built_in">range</span>(<span class="built_in">len</span>(self.vocab))))</span><br><span class="line">            unk_index = index[<span class="string">&#x27;$UNK&#x27;</span>]</span><br><span class="line">            <span class="keyword">for</span> ex <span class="keyword">in</span> X:</span><br><span class="line">                seq = [index.get(w, unk_index) <span class="keyword">for</span> w <span class="keyword">in</span> ex]</span><br><span class="line">                seq = torch.tensor(seq)</span><br><span class="line">                new_X.append(seq)</span><br><span class="line">                seq_lengths.append(<span class="built_in">len</span>(seq))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_X = [torch.FloatTensor(ex) <span class="keyword">for</span> ex <span class="keyword">in</span> X]</span><br><span class="line">            seq_lengths = [<span class="built_in">len</span>(ex) <span class="keyword">for</span> ex <span class="keyword">in</span> X]</span><br><span class="line">            self.embed_dim = X[<span class="number">0</span>][<span class="number">0</span>].shape[<span class="number">0</span>]</span><br><span class="line">        seq_lengths = torch.tensor(seq_lengths)</span><br><span class="line">        <span class="keyword">return</span> new_X, seq_lengths</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">self, X, y, device=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Uses macro-F1 as the score function. Note: this departs from</span></span><br><span class="line"><span class="string">        `sklearn`, where classifiers use accuracy as their scoring</span></span><br><span class="line"><span class="string">        function. Using macro-F1 is more consistent with our course.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        This function can be used to evaluate models, but its primary</span></span><br><span class="line"><span class="string">        use is in cross-validation and hyperparameter tuning.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X: np.array, shape `(n_examples, n_features)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y: iterable, shape `len(n_examples)`</span></span><br><span class="line"><span class="string">            These can be the raw labels. They will converted internally</span></span><br><span class="line"><span class="string">            as needed. See `build_dataset`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        device: str or None</span></span><br><span class="line"><span class="string">            Allows the user to temporarily change the device used</span></span><br><span class="line"><span class="string">            during prediction. This is useful if predictions require a</span></span><br><span class="line"><span class="string">            lot of memory and so are better done on the CPU. After</span></span><br><span class="line"><span class="string">            prediction is done, the model is returned to `self.device`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        float</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        preds = self.predict(X, device=device)</span><br><span class="line">        <span class="keyword">return</span> utils.safe_macro_f1(y, preds)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict_proba</span>(<span class="params">self, X, device=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Predicted probabilities for the examples in `X`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : np.array, shape `(n_examples, n_features)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        device: str or None</span></span><br><span class="line"><span class="string">            Allows the user to temporarily change the device used</span></span><br><span class="line"><span class="string">            during prediction. This is useful if predictions require a</span></span><br><span class="line"><span class="string">            lot of memory and so are better done on the CPU. After</span></span><br><span class="line"><span class="string">            prediction is done, the model is returned to `self.device`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        np.array, shape `(len(X), self.n_classes_)`</span></span><br><span class="line"><span class="string">            Each row of this matrix will sum to 1.0.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        preds = self._predict(X, device=device)</span><br><span class="line">        probs = torch.softmax(preds, dim=<span class="number">1</span>).cpu().numpy()</span><br><span class="line">        <span class="keyword">return</span> probs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X, device=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Predicted labels for the examples in `X`. These are converted</span></span><br><span class="line"><span class="string">        from the integers that PyTorch needs back to their original</span></span><br><span class="line"><span class="string">        values in `self.classes_`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : np.array, shape `(n_examples, n_features)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        device: str or None</span></span><br><span class="line"><span class="string">            Allows the user to temporarily change the device used</span></span><br><span class="line"><span class="string">            during prediction. This is useful if predictions require a</span></span><br><span class="line"><span class="string">            lot of memory and so are better done on the CPU. After</span></span><br><span class="line"><span class="string">            prediction is done, the model is returned to `self.device`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        list, length len(X)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        probs = self.predict_proba(X, device=device)</span><br><span class="line">        <span class="keyword">return</span> [self.classes_[i] <span class="keyword">for</span> i <span class="keyword">in</span> probs.argmax(axis=<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simple_example</span>():</span><br><span class="line">    utils.fix_random_seeds()</span><br><span class="line"></span><br><span class="line">    vocab = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;$UNK&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># No b before an a</span></span><br><span class="line">    train = [</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;ab&#x27;</span>), <span class="string">&#x27;good&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;aab&#x27;</span>), <span class="string">&#x27;good&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;abb&#x27;</span>), <span class="string">&#x27;good&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;aabb&#x27;</span>), <span class="string">&#x27;good&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;ba&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;baa&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;bba&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;bbaa&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;aba&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">    test = [</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;baaa&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;abaa&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;bbaa&#x27;</span>), <span class="string">&#x27;bad&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;aaab&#x27;</span>), <span class="string">&#x27;good&#x27;</span>],</span><br><span class="line">        [<span class="built_in">list</span>(<span class="string">&#x27;aaabb&#x27;</span>), <span class="string">&#x27;good&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">    X_train, y_train = <span class="built_in">zip</span>(*train)</span><br><span class="line">    X_test, y_test = <span class="built_in">zip</span>(*test)</span><br><span class="line"></span><br><span class="line">    mod = TorchRNNClassifier(vocab)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(mod)</span><br><span class="line"></span><br><span class="line">    mod.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    preds = mod.predict(X_test)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nPredictions:&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ex, pred, gold <span class="keyword">in</span> <span class="built_in">zip</span>(X_test, preds, y_test):</span><br><span class="line">        score = <span class="string">&quot;correct&quot;</span> <span class="keyword">if</span> pred == gold <span class="keyword">else</span> <span class="string">&quot;incorrect&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;&#123;0:&gt;6&#125; - predicted: &#123;1:&gt;4&#125;; actual: &#123;2:&gt;4&#125; - &#123;3&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">            <span class="string">&quot;&quot;</span>.join(ex), pred, gold, score))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mod.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    simple_example()</span><br></pre></td></tr></table></figure>
<p>This is a potential gain over our average-vectors baseline, in that it processes each word independently, and in the context of those that came before it. Thus, not only is this sensitive to word order, but the hidden representation create the potential to encode how the preceding context for a word affects its interpretation.</p>
<p>The downside of this, of course, is that this model is much more difficult to set up and optimize. Let’s dive into those details.</p>
<h3 id="RNN-dataset-preparation"><a href="#RNN-dataset-preparation" class="headerlink" title="RNN dataset preparation"></a>RNN dataset preparation</h3><p>SST contains trees, but the RNN processes just the sequence of leaf nodes. The function <code>build_rnn_dataset</code> creates datasets in this format:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_rnn_dataset</span>(<span class="params">dataframes, tokenizer=<span class="keyword">lambda</span> s: s.split(<span class="params"></span>)</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Given an SST reader, return the dataset as (X, y) training pairs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    dataframes : pd.DataFrame or list of pd.DataFrame</span></span><br><span class="line"><span class="string">        The dataset or datasets to process, as read in by</span></span><br><span class="line"><span class="string">        `sentiment_reader`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    tokenizer : function from str to list of str</span></span><br><span class="line"><span class="string">        Defaults to a whitespace tokenizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    X, y</span></span><br><span class="line"><span class="string">        Where X is a list of list of str, and y is the output label list.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(dataframes, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">        df = pd.concat(dataframes)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        df = dataframes</span><br><span class="line">    X = <span class="built_in">list</span>(df.sentence.apply(tokenizer))</span><br><span class="line">    y = <span class="built_in">list</span>(df.label.values)</span><br><span class="line">    <span class="keyword">return</span> X, y</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_rnn_train, y_rnn_train = build_rnn_dataset(train_reader(SST_HOME))</span><br><span class="line">X_rnn_dev, y_rnn_dev = build_rnn_dataset(dev_reader(SST_HOME))</span><br></pre></td></tr></table></figure>
<h3 id="Vocabulary-for-the-embedding"><a href="#Vocabulary-for-the-embedding" class="headerlink" title="Vocabulary for the embedding"></a>Vocabulary for the embedding</h3><p>The first delicate issue we need to address is the vocabulary for our model:</p>
<ul>
<li>As indicated in the figure above, the first thing we do when  processing an example is look up the words in an embedding (a VSM),  which has to have a fixed dimensionality. </li>
<li>We can use our training data to specify the vocabulary for this  embedding; at prediction time, though, we will inevitably encounter  words we haven’t seen before. </li>
<li>The convention we adopt here is to map them to an <code>$UNK</code> token that is in our pre-specified vocabulary.</li>
<li>At the same time, we might want to collapse infrequent tokens into <code>$UNK</code> to make optimization easier and to try to create reasonable representations for words that we have to map to <code>$UNK</code> at test time.</li>
</ul>
<p>In <code>utils</code>, the function <code>get_vocab</code> will help you specify a vocabulary. It will let you choose a vocabulary by optionally specifying <code>mincount</code> or <code>n_words</code>, and it will ensure that <code>$UNK</code> is included.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_vocab</span>(<span class="params">X, n_words=<span class="literal">None</span>, mincount=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Get the vocabulary for an RNN example matrix `X`, adding $UNK$ if</span></span><br><span class="line"><span class="string">    it isn&#x27;t already present.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X : list of lists of str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    n_words : int or None</span></span><br><span class="line"><span class="string">        If this is `int &gt; 0`, keep only the top `n_words` by frequency.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    mincount : int</span></span><br><span class="line"><span class="string">        Only words with at least this many tokens are kept.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    list of str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    wc = Counter([w <span class="keyword">for</span> ex <span class="keyword">in</span> X <span class="keyword">for</span> w <span class="keyword">in</span> ex])</span><br><span class="line">    wc = wc.most_common(n_words) <span class="keyword">if</span> n_words <span class="keyword">else</span> wc.items()</span><br><span class="line">    <span class="keyword">if</span> mincount &gt; <span class="number">1</span>:</span><br><span class="line">        wc = &#123;(w, c) <span class="keyword">for</span> w, c <span class="keyword">in</span> wc <span class="keyword">if</span> c &gt;= mincount&#125;</span><br><span class="line">    vocab = &#123;w <span class="keyword">for</span> w, _ <span class="keyword">in</span> wc&#125;</span><br><span class="line">    vocab.add(<span class="string">&quot;$UNK&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sorted</span>(vocab)</span><br></pre></td></tr></table></figure>
<h3 id="PyTorch-RNN-classifier"><a href="#PyTorch-RNN-classifier" class="headerlink" title="PyTorch RNN classifier"></a>PyTorch RNN classifier</h3><p>Here and throughout, we’ll rely on <code>early_stopping=True</code> to try to find the optimal time to stop optimization. This behavior can be further refined by setting different values of <code>validation_fraction</code>, <code>n_iter_no_change</code>, and <code>tol</code>. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_rnn_classifier <span class="keyword">import</span> TorchRNNClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">rnn = TorchRNNClassifier(</span><br><span class="line">    sst_train_vocab,</span><br><span class="line">    early_stopping=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">%time _ = rnn.fit(X_rnn_train, y_rnn_train)</span><br><span class="line"></span><br><span class="line">rnn_dev_preds = rnn.predict(X_rnn_dev)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_rnn_dev, rnn_dev_preds, digits=<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220511161449792.png" class title="图片">
<h3 id="Pretrained-embeddings"><a href="#Pretrained-embeddings" class="headerlink" title="Pretrained embeddings"></a>Pretrained embeddings</h3><p>With <code>embedding=None</code>, <code>TorchRNNClassifier</code> (and its counterpart in <code>np_rnn_classifier.py</code>) create random embeddings. You can also pass in an embedding, as long as you make sure it has the right vocabulary. The utility <code>utils.create_pretrained_embedding</code> will help with that:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_pretrained_embedding</span>(<span class="params"></span></span><br><span class="line"><span class="params">        lookup, vocab, required_tokens=(<span class="params"><span class="string">&#x27;$UNK&#x27;</span>, <span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span></span>)</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Create an embedding matrix from a lookup and a specified vocab.</span></span><br><span class="line"><span class="string">    Words from `vocab` that are not in `lookup` are given random</span></span><br><span class="line"><span class="string">    representations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    lookup : dict</span></span><br><span class="line"><span class="string">        Must map words to their vector representations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    vocab : list of str</span></span><br><span class="line"><span class="string">        Words to create embeddings for.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    required_tokens : tuple of str</span></span><br><span class="line"><span class="string">        Tokens that must have embeddings. If they are not available</span></span><br><span class="line"><span class="string">        in the look-up, they will be given random representations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    np.array, list</span></span><br><span class="line"><span class="string">        The np.array is an embedding for `vocab` and the `list` is</span></span><br><span class="line"><span class="string">        the potentially expanded version of `vocab` that came in.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dim = <span class="built_in">len</span>(<span class="built_in">next</span>(<span class="built_in">iter</span>(lookup.values())))</span><br><span class="line">    embedding = np.array([lookup.get(w, randvec(dim)) <span class="keyword">for</span> w <span class="keyword">in</span> vocab])</span><br><span class="line">    <span class="keyword">for</span> tok <span class="keyword">in</span> required_tokens:</span><br><span class="line">        <span class="keyword">if</span> tok <span class="keyword">not</span> <span class="keyword">in</span> vocab:</span><br><span class="line">            vocab.append(tok)</span><br><span class="line">            embedding = np.vstack((embedding, randvec(dim)))</span><br><span class="line">    <span class="keyword">return</span> embedding, vocab</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">glove_embedding, sst_glove_vocab = create_pretrained_embedding(</span><br><span class="line">    glove_lookup, sst_train_vocab)</span><br><span class="line"></span><br><span class="line">rnn_glove = TorchRNNClassifier(</span><br><span class="line">    sst_glove_vocab,</span><br><span class="line">    embedding=glove_embedding,</span><br><span class="line">    early_stopping=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">%time _ = rnn_glove.fit(X_rnn_train, y_rnn_train)</span><br><span class="line"></span><br><span class="line">rnn_glove_dev_preds = rnn_glove.predict(X_rnn_dev)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_rnn_dev, rnn_glove_dev_preds, digits=<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220511161644446.png" class title="图片">
<h3 id="RNN-hyperparameter-tuning-experiment"><a href="#RNN-hyperparameter-tuning-experiment" class="headerlink" title="RNN hyperparameter tuning experiment"></a>RNN hyperparameter tuning experiment</h3><p>We’re not really done until we’ve done some hyperparameter search. So  let’s round out this section by cross-validating the RNN that uses GloVe embeddings, to see if we can improve on the default-parameters model we evaluated just above. For this, we’ll use <code>sst.experiment</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">simple_leaves_phi</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="keyword">return</span> text.split()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit_rnn_with_hyperparameter_search</span>(<span class="params">X, y</span>):</span><br><span class="line">    basemod = TorchRNNClassifier(</span><br><span class="line">        sst_train_vocab,</span><br><span class="line">        embedding=glove_embedding,</span><br><span class="line">        batch_size=<span class="number">25</span>,  <span class="comment"># Inspired by comments in the paper.</span></span><br><span class="line">        bidirectional=<span class="literal">True</span>,</span><br><span class="line">        early_stopping=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># There are lots of other parameters and values we could</span></span><br><span class="line">    <span class="comment"># explore, but this is at least a solid start:</span></span><br><span class="line">    param_grid = &#123;</span><br><span class="line">        <span class="string">&#x27;embed_dim&#x27;</span>: [<span class="number">50</span>, <span class="number">75</span>, <span class="number">100</span>],</span><br><span class="line">        <span class="string">&#x27;hidden_dim&#x27;</span>: [<span class="number">50</span>, <span class="number">75</span>, <span class="number">100</span>],</span><br><span class="line">        <span class="string">&#x27;eta&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>]&#125;</span><br><span class="line"></span><br><span class="line">    bestmod = utils.fit_classifier_with_hyperparameter_search(</span><br><span class="line">        X, y, basemod, cv=<span class="number">3</span>, param_grid=param_grid)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bestmod</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">rnn_experiment_xval = experiment(</span><br><span class="line">    train_reader(SST_HOME),</span><br><span class="line">    simple_leaves_phi,</span><br><span class="line">    fit_rnn_with_hyperparameter_search,</span><br><span class="line">    assess_dataframes=dev_reader(SST_HOME),</span><br><span class="line">    vectorize=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220511162118858.png" class title="图片">
<h2 id="The-VecAvg-baseline-from-Socher-et-al-2013"><a href="#The-VecAvg-baseline-from-Socher-et-al-2013" class="headerlink" title="The VecAvg baseline from Socher et al. 2013"></a>The VecAvg baseline from Socher et al. 2013</h2><p>One of the baseline models from <a target="_blank" rel="noopener" href="http://www.aclweb.org/anthology/D/D13/D13-1170.pdf">Socher et al., Table 1</a> is <strong>VecAvg</strong>. This is like the model we explored above under the heading of <a href="#Distributed-representations-as-features">Distributed representations as features</a>, but it uses a random initial embedding that is updated as part of optimization. Another perspective on it is that it is like the RNN we just evaluated, but with the RNN parameters replaced by averaging. </p>
<p>In Socher et al. 2013, this model does reasonably well, scoring 80.1 on the root-only binary problem. In this section, we reimplement it, relying on <code>TorchRNNClassifier</code> to handle most of the heavy-lifting, and we evaluate it with a reasonably wide hyperparameter search.</p>
<h3 id="Defining-the-model"><a href="#Defining-the-model" class="headerlink" title="Defining the model"></a>Defining the model</h3><p>The core model is <code>TorchVecAvgModel</code>, which just looks up embeddings, averages them, and feeds the result to a classifier layer:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TorchVecAvgModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, output_dim, device, embed_dim=<span class="number">50</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        self.device = device</span><br><span class="line">        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)</span><br><span class="line">        self.classifier_layer = nn.Linear(self.embed_dim, self.output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, seq_lengths</span>):</span><br><span class="line">        embs = self.embedding(X)</span><br><span class="line">        <span class="comment"># Mask based on the **true** lengths:</span></span><br><span class="line">        mask = [torch.ones(l, self.embed_dim) <span class="keyword">for</span> l <span class="keyword">in</span> seq_lengths]</span><br><span class="line">        mask = torch.nn.utils.rnn.pad_sequence(mask, batch_first=<span class="literal">True</span>)</span><br><span class="line">        mask = mask.to(self.device)</span><br><span class="line">        <span class="comment"># True average:</span></span><br><span class="line">        mu = (embs * mask).<span class="built_in">sum</span>(axis=<span class="number">1</span>) / seq_lengths.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># Classifier:</span></span><br><span class="line">        logits = self.classifier_layer(mu)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<p>For the main interface, we can just subclass <code>TorchRNNClassifier</code> and change the <code>build_graph</code> method to use <code>TorchVecAvgModel</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TorchVecAvgClassifier</span>(<span class="title class_ inherited__">TorchRNNClassifier</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_graph</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> TorchVecAvgModel(</span><br><span class="line">            vocab_size=<span class="built_in">len</span>(self.vocab),</span><br><span class="line">            output_dim=self.n_classes_,</span><br><span class="line">            device=self.device,</span><br><span class="line">            embed_dim=self.embed_dim)</span><br></pre></td></tr></table></figure>
<h3 id="VecAvg-hyperparameter-tuning-experiment"><a href="#VecAvg-hyperparameter-tuning-experiment" class="headerlink" title="VecAvg hyperparameter tuning experiment"></a>VecAvg hyperparameter tuning experiment</h3><p>Now that we have the model implemented, let’s see if we can reproduce  Socher et al.’s 80.1 on the binary, root-only version of SST.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">train_df = train_reader(SST_HOME)</span><br><span class="line"></span><br><span class="line">train_bin_df = train_df[train_df.label != <span class="string">&#x27;neutral&#x27;</span>]</span><br><span class="line"></span><br><span class="line">dev_df = dev_reader(SST_HOME)</span><br><span class="line"></span><br><span class="line">dev_bin_df = dev_df[dev_df.label != <span class="string">&#x27;neutral&#x27;</span>]</span><br><span class="line"></span><br><span class="line">test_df = sentiment_reader(os.path.join(SST_HOME, <span class="string">&quot;sst3-test-labeled.csv&quot;</span>))</span><br><span class="line"></span><br><span class="line">test_bin_df = test_df[test_df.label != <span class="string">&#x27;neutral&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit_vecavg_with_hyperparameter_search</span>(<span class="params">X, y</span>):</span><br><span class="line">    basemod = TorchVecAvgClassifier(</span><br><span class="line">        sst_train_vocab,</span><br><span class="line">        early_stopping=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    param_grid = &#123;</span><br><span class="line">        <span class="string">&#x27;embed_dim&#x27;</span>: [<span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>],</span><br><span class="line">        <span class="string">&#x27;eta&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.05</span>]&#125;</span><br><span class="line"></span><br><span class="line">    bestmod = utils.fit_classifier_with_hyperparameter_search(</span><br><span class="line">        X, y, basemod, cv=<span class="number">3</span>, param_grid=param_grid)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bestmod</span><br><span class="line"></span><br><span class="line">%%time</span><br><span class="line">vecavg_experiment_xval = sst.experiment(</span><br><span class="line">    [train_bin_df, dev_bin_df],</span><br><span class="line">    simple_leaves_phi,</span><br><span class="line">    fit_vecavg_with_hyperparameter_search,</span><br><span class="line">    assess_dataframes=test_bin_df,</span><br><span class="line">    vectorize=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<img src="/2022/05/11/SST/image-20220511162628067.png" class title="图片">
<p>It looks like we basically reproduced the number from the paper (80.1).</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/markdown/" rel="tag"># markdown</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/05/11/%E5%88%A9%E7%94%A8%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE/" rel="prev" title="利用社交网络数据">
      <i class="fa fa-chevron-left"></i> 利用社交网络数据
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Supervised-sentiment-analysis"><span class="nav-number">1.</span> <span class="nav-text">Supervised sentiment analysis</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Overview"><span class="nav-number">2.</span> <span class="nav-text">Overview</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Conceptual-challenges"><span class="nav-number">2.1.</span> <span class="nav-text">Conceptual challenges</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sentiment-analysis-in-industry"><span class="nav-number">2.2.</span> <span class="nav-text">Sentiment analysis in industry</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Affective-computing"><span class="nav-number">2.3.</span> <span class="nav-text">Affective computing</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#General-practical-tips"><span class="nav-number">3.</span> <span class="nav-text">General practical tips</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Selected-sentiment-datasets"><span class="nav-number">3.1.</span> <span class="nav-text">Selected sentiment datasets</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lexica"><span class="nav-number">3.2.</span> <span class="nav-text">Lexica</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tokenizing"><span class="nav-number">3.3.</span> <span class="nav-text">Tokenizing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Whitespace-tokenizer"><span class="nav-number">3.3.1.</span> <span class="nav-text">Whitespace tokenizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Treebank-tokenizer"><span class="nav-number">3.3.2.</span> <span class="nav-text">Treebank tokenizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sentiment-aware-tokenizer"><span class="nav-number">3.3.3.</span> <span class="nav-text">Sentiment-aware tokenizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-impact-of-sentiment-aware-tokenizing"><span class="nav-number">3.3.4.</span> <span class="nav-text">The impact of sentiment-aware tokenizing</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-dangers-of-stemming"><span class="nav-number">3.4.</span> <span class="nav-text">The dangers of stemming</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Porter-stemmer"><span class="nav-number">3.4.1.</span> <span class="nav-text">The Porter stemmer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Lancaster-stemmer"><span class="nav-number">3.4.2.</span> <span class="nav-text">The Lancaster stemmer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-WordNet-stemmer"><span class="nav-number">3.4.3.</span> <span class="nav-text">The WordNet stemmer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-impact-of-stemming"><span class="nav-number">3.4.4.</span> <span class="nav-text">The impact of stemming</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Other-preprocessing-techniques"><span class="nav-number">3.5.</span> <span class="nav-text">Other preprocessing techniques</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Part-of-speech-POS-tagging"><span class="nav-number">3.5.1.</span> <span class="nav-text">Part-of-speech (POS) tagging</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-limits-of-POS-tagging"><span class="nav-number">3.5.2.</span> <span class="nav-text">The limits of POS tagging</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Simple-negation-marking"><span class="nav-number">3.5.3.</span> <span class="nav-text">Simple negation marking</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-impact-of-negation-marking"><span class="nav-number">3.5.4.</span> <span class="nav-text">The impact of negation marking</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Stanford-Sentiment-Treebank"><span class="nav-number">4.</span> <span class="nav-text">Stanford Sentiment Treebank</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SST-project-overview"><span class="nav-number">4.1.</span> <span class="nav-text">SST project overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Root-level-tasks"><span class="nav-number">4.2.</span> <span class="nav-text">Root-level tasks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Train-dev-test-scenarios"><span class="nav-number">4.3.</span> <span class="nav-text">Train&#x2F;dev&#x2F;test scenarios</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Code-Snips"><span class="nav-number">4.4.</span> <span class="nav-text">Code Snips</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SST-Data-readers"><span class="nav-number">4.4.1.</span> <span class="nav-text">SST Data readers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SST-Tokenization"><span class="nav-number">4.4.2.</span> <span class="nav-text">SST Tokenization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DynaSent"><span class="nav-number">5.</span> <span class="nav-text">DynaSent</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Project-overview"><span class="nav-number">5.1.</span> <span class="nav-text">Project overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dataset-overview"><span class="nav-number">5.2.</span> <span class="nav-text">Dataset overview</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Round-1"><span class="nav-number">5.2.1.</span> <span class="nav-text">Round 1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Round-2"><span class="nav-number">5.2.2.</span> <span class="nav-text">Round 2</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hyperparameter-search-and-classififier-comparison"><span class="nav-number">6.</span> <span class="nav-text">Hyperparameter search and classififier comparison</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Hyperparameter-search-Rationale"><span class="nav-number">6.1.</span> <span class="nav-text">Hyperparameter search : Rationale</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Classififier-comparison-Rationale"><span class="nav-number">6.2.</span> <span class="nav-text">Classififier comparison: Rationale</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hand-built-feature-functions"><span class="nav-number">7.</span> <span class="nav-text">Hand-built feature functions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Overview-1"><span class="nav-number">7.1.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Feature-functions"><span class="nav-number">7.2.</span> <span class="nav-text">Feature functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Unigrams"><span class="nav-number">7.2.1.</span> <span class="nav-text">Unigrams</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bigrams"><span class="nav-number">7.2.2.</span> <span class="nav-text">Bigrams</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-note-on-DictVectorizer"><span class="nav-number">7.2.3.</span> <span class="nav-text">A note on DictVectorizer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Building-datasets-for-experiments"><span class="nav-number">7.3.</span> <span class="nav-text">Building datasets for experiments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic-optimization"><span class="nav-number">7.4.</span> <span class="nav-text">Basic optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Wrapper-for-SGDClassifier"><span class="nav-number">7.4.1.</span> <span class="nav-text">Wrapper for SGDClassifier</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Wrapper-for-LogisticRegression"><span class="nav-number">7.4.2.</span> <span class="nav-text">Wrapper for LogisticRegression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Wrapper-for-TorchShallowNeuralClassifier"><span class="nav-number">7.4.3.</span> <span class="nav-text">Wrapper for TorchShallowNeuralClassifier</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-softmax-classifier-in-PyTorch"><span class="nav-number">7.4.4.</span> <span class="nav-text">A softmax classifier in PyTorch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Using-sklearn-Pipelines"><span class="nav-number">7.4.5.</span> <span class="nav-text">Using sklearn Pipelines</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hyperparameter-search"><span class="nav-number">7.5.</span> <span class="nav-text">Hyperparameter search</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#fit-classifier-with-hyperparameter-search"><span class="nav-number">7.5.1.</span> <span class="nav-text">fit_classifier_with_hyperparameter_search</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Example-using-LogisticRegression"><span class="nav-number">7.5.2.</span> <span class="nav-text">Example using LogisticRegression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reproducing-baselines-from-Socher-et-al-2013"><span class="nav-number">7.6.</span> <span class="nav-text">Reproducing  baselines from Socher et al. 2013</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Reproducing-the-Unigram-NaiveBayes-results"><span class="nav-number">7.6.1.</span> <span class="nav-text">Reproducing the Unigram NaiveBayes results</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reproducing-the-Bigrams-NaiveBayes-results"><span class="nav-number">7.6.2.</span> <span class="nav-text">Reproducing the Bigrams NaiveBayes results</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reproducing-the-SVM-results"><span class="nav-number">7.6.3.</span> <span class="nav-text">Reproducing the SVM results</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Statistical-comparison-of-classifier-models"><span class="nav-number">7.7.</span> <span class="nav-text">Statistical comparison of classifier models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Comparison-with-the-Wilcoxon-signed-rank-test"><span class="nav-number">7.7.1.</span> <span class="nav-text">Comparison with the Wilcoxon signed-rank test</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Comparison-with-McNemar%E2%80%99s-test"><span class="nav-number">7.7.2.</span> <span class="nav-text">Comparison with McNemar’s test</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Feature-representation"><span class="nav-number">8.</span> <span class="nav-text">Feature representation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#N-gram-feature-functions"><span class="nav-number">8.1.</span> <span class="nav-text">N-gram feature functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Other-ideas-for-hand-built-feature-functions"><span class="nav-number">8.2.</span> <span class="nav-text">Other ideas for hand-built feature functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Assessing-individual-feature-functions"><span class="nav-number">8.3.</span> <span class="nav-text">Assessing individual feature functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Distributed-representations-as-features"><span class="nav-number">8.4.</span> <span class="nav-text">Distributed representations as features</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RNN-classifiers"><span class="nav-number">9.</span> <span class="nav-text">RNN classifiers</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-overview"><span class="nav-number">9.1.</span> <span class="nav-text">Model overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-note-on-LSTMs"><span class="nav-number">9.2.</span> <span class="nav-text">A note on LSTMs</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dense-feature-representations-and-neural-networks"><span class="nav-number">10.</span> <span class="nav-text">Dense feature representations and neural networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Overview-2"><span class="nav-number">10.1.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Distributed-representations-as-features-1"><span class="nav-number">10.2.</span> <span class="nav-text">Distributed representations as features</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GloVe-inputs"><span class="nav-number">10.2.1.</span> <span class="nav-text">GloVe inputs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Yelp-representations"><span class="nav-number">10.2.2.</span> <span class="nav-text">Yelp representations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Remarks-on-this-approach"><span class="nav-number">10.2.3.</span> <span class="nav-text">Remarks on this approach</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN-classifiers-1"><span class="nav-number">10.3.</span> <span class="nav-text">RNN classifiers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN-dataset-preparation"><span class="nav-number">10.3.1.</span> <span class="nav-text">RNN dataset preparation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vocabulary-for-the-embedding"><span class="nav-number">10.3.2.</span> <span class="nav-text">Vocabulary for the embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PyTorch-RNN-classifier"><span class="nav-number">10.3.3.</span> <span class="nav-text">PyTorch RNN classifier</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pretrained-embeddings"><span class="nav-number">10.3.4.</span> <span class="nav-text">Pretrained embeddings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN-hyperparameter-tuning-experiment"><span class="nav-number">10.3.5.</span> <span class="nav-text">RNN hyperparameter tuning experiment</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-VecAvg-baseline-from-Socher-et-al-2013"><span class="nav-number">10.4.</span> <span class="nav-text">The VecAvg baseline from Socher et al. 2013</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Defining-the-model"><span class="nav-number">10.4.1.</span> <span class="nav-text">Defining the model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VecAvg-hyperparameter-tuning-experiment"><span class="nav-number">10.4.2.</span> <span class="nav-text">VecAvg hyperparameter tuning experiment</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Lee</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="/853784202@qq.com" title="E-Mail → 853784202@qq.com"><i class="lee-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>



<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="lee"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lee</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="lee-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="lee-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
